{
  "cluster_id": 8,
  "papers": [
    {
      "id": "cvpr23_1397",
      "paper_id": "",
      "title": "(ML)$^2$P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning",
      "authors": "Ziming Liu, Song Guo, Xiaocheng Lu, Jingcai Guo, Jiewei Zhang, Yue Zeng, Fushuo Huo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_112",
      "paper_id": "",
      "title": "1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions",
      "authors": "Dongshuo Yin, Yiran Yang, Zhechao Wang, Hongfeng Yu, Kaiwen Wei, Xian Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_823",
      "paper_id": "",
      "title": "5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks",
      "authors": "Dongshuo Yin, Leiyi Hu, Bin Li, Youqun Zhang, Xue Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_5100_Breaking_Performance_Shackles_of_Full_Fine-Tuning_on_Visual_Recognition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1505",
      "paper_id": "",
      "title": "A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models",
      "authors": "Julio Silva-Rodríguez, Sina Hajimiri, Ismail Ben Ayed, Jose Dolz",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Silva-Rodriguez_A_Closer_Look_at_the_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2195",
      "paper_id": "",
      "title": "A Generative Approach for Wikipedia-Scale Visual Entity Recognition",
      "authors": "Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Caron_A_Generative_Approach_for_Wikipedia-Scale_Visual_Entity_Recognition_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2018",
      "paper_id": "",
      "title": "A New Benchmark: On the Utility of Synthetic Data With Blender for Bare Supervised Learning and Downstream Domain Adaptation",
      "authors": "Hui Tang, Kui Jia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_A_New_Benchmark_On_the_Utility_of_Synthetic_Data_With_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_728",
      "paper_id": "",
      "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs",
      "authors": "Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, Yang You",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_A_Stitch_in_Time_Saves_Nine_Small_VLM_is_a_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_806",
      "paper_id": "",
      "title": "Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction",
      "authors": "Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, Licheng Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Accelerating_Multimodal_Large_Language_Models_by_Searching_Optimal_Vision_Token_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_137",
      "paper_id": "",
      "title": "Accelerating Vision-Language Pretraining With Free Language Modeling",
      "authors": "Teng Wang, Yixiao Ge, Feng Zheng, Ran Cheng, Ying Shan, Xiaohu Qie, Ping Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1071",
      "paper_id": "",
      "title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models",
      "authors": "Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, Olivier J. Henaff",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Udandarao_Active_Data_Curation_Effectively_Distills_Large-Scale_Multimodal_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1522",
      "paper_id": "",
      "title": "Active Prompt Learning in Vision Language Models",
      "authors": "Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bang_Active_Prompt_Learning_in_Vision_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2796",
      "paper_id": "",
      "title": "AdaDARE-gamma: Balancing Stability and Plasticity in Multi-modal LLMs through Efficient Adaptation",
      "authors": "Jingyi Xie, Jintao Yang, Zhunchen Luo, Yunbo Cao, Qiang Gao, Mengyuan Zhang, Wenpeng Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_AdaDARE-gamma_Balancing_Stability_and_Plasticity_in_Multi-modal_LLMs_through_Efficient_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2717",
      "paper_id": "",
      "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
      "authors": "Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Zhifang Sui, Maosong Sun, Yang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Du_AdaMMS_Model_Merging_for_Heterogeneous_Multimodal_Large_Language_Models_with_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1128",
      "paper_id": "",
      "title": "Adapters Strike Back",
      "authors": "Jan-Martin O. Steitz, Stefan Roth",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Steitz_Adapters_Strike_Back_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_610",
      "paper_id": "",
      "title": "Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition",
      "authors": "Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Jin Li, Yuchen Liu, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_756",
      "paper_id": "",
      "title": "Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval",
      "authors": "Yang Liu, Qingchao Chen, Samuel Albanie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Adaptive_Cross-Modal_Prototypes_for_Cross-Domain_Visual-Language_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2410",
      "paper_id": "",
      "title": "Adaptive Parameter Selection for Tuning Vision-Language Models",
      "authors": "Yi Zhang, Yi-Xuan Deng, Meng-Hao Guo, Shi-Min Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Adaptive_Parameter_Selection_for_Tuning_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1264",
      "paper_id": "",
      "title": "Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition",
      "authors": "Chengxiang Huang, Yake Wei, Zequn Yang, Di Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Adaptive_Unimodal_Regulation_for_Balanced_Multimodal_Information_Acquisition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2351",
      "paper_id": "",
      "title": "AdMiT: Adaptive Multi-Source Tuning in Dynamic Environments",
      "authors": "Xiangyu Chang, Fahim Faisal Niloy, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler, Ananthram Swami, Samet Oymak, Amit Roy-Chowdhury",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_AdMiT_Adaptive_Multi-Source_Tuning_in_Dynamic_Environments_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_910",
      "paper_id": "",
      "title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training",
      "authors": "Haicheng Wang, Chen Ju, Weixiong Lin, Shuai Xiao, Mengting Chen, Yixuan Huang, Chang Liu, Mingshuai Yao, Jinsong Lan, Ying Chen, Qingwen Liu, Yanfeng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Advancing_Myopia_To_Holism_Fully_Contrastive_Language-Image_Pre-training_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1696",
      "paper_id": "",
      "title": "Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement",
      "authors": "Qianhan Feng, Wenshuo Li, Tong Lin, Xinghao Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_Align-KD_Distilling_Cross-Modal_Alignment_Knowledge_for_Mobile_Vision-Language_Large_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_479",
      "paper_id": "",
      "title": "Aligning and Prompting Everything All at Once for Universal Visual Perception",
      "authors": "Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Aligning_and_Prompting_Everything_All_at_Once_for_Universal_Visual_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_63",
      "paper_id": "",
      "title": "Aligning Bag of Regions for Open-Vocabulary Object Detection",
      "authors": "Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_393",
      "paper_id": "",
      "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
      "authors": "Yan Li, Yifei Xing, Xiangyuan Lan, Xin Li, Haifeng Chen, Dongmei Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_AlignMamba_Enhancing_Multimodal_Mamba_with_Local_and_Global_Cross-modal_Alignment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_322",
      "paper_id": "",
      "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
      "authors": "Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_928",
      "paper_id": "",
      "title": "AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into One",
      "authors": "Mike Ranzinger, Greg Heinrich, Jan Kautz, Pavlo Molchanov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ranzinger_AM-RADIO_Agglomerative_Vision_Foundation_Model_Reduce_All_Domains_Into_One_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1557",
      "paper_id": "",
      "title": "AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning",
      "authors": "Yuwei Tang, Zhenyi Lin, Qilong Wang, Pengfei Zhu, Qinghua Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_AMU-Tuning_Effective_Logit_Bias_for_CLIP-based_Few-shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1372",
      "paper_id": "",
      "title": "An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing",
      "authors": "Feiran Hu, Chenlin Zhang, Jiangliang Guo, Xiu-Shen Wei, Lin Zhao, Anqi Xu, Lingyan Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_An_Asymmetric_Augmented_Self-Supervised_Learning_Method_for_Unsupervised_Fine-Grained_Image_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1338",
      "paper_id": "",
      "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
      "authors": "Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1886",
      "paper_id": "",
      "title": "Any-Shift Prompting for Generalization over Distributions",
      "authors": "Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Any-Shift_Prompting_for_Generalization_over_Distributions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1778",
      "paper_id": "",
      "title": "Are Multimodal Transformers Robust to Missing Modality?",
      "authors": "Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, Xi Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2214",
      "paper_id": "",
      "title": "ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models",
      "authors": "Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_ArGue_Attribute-Guided_Prompt_Tuning_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1189",
      "paper_id": "",
      "title": "Argus: A Compact and Versatile Foundation Model for Vision",
      "authors": "Weiming Zhuang, Chen Chen, Zhizhong Li, Sina Sajadmanesh, Jingtao Li, Jiabo Huang, Vikash Sehwag, Vivek Sharma, Hirotaka Shinozaki, Felan Carlo Garcia, Yihao Zhan, Naohiro Adachi, Ryoji Eki, Michael Spranger, Peter Stone, Lingjuan Lyu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhuang_Argus_A_Compact_and_Versatile_Foundation_Model_for_Vision_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1035",
      "paper_id": "",
      "title": "Assessing and Learning Alignment of Unimodal Vision and Language Models",
      "authors": "Le Zhang, Qian Yang, Aishwarya Agrawal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_325",
      "paper_id": "",
      "title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models",
      "authors": "Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, Yansong Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_ATP-LLaVA_Adaptive_Token_Pruning_for_Large_Vision_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1022",
      "paper_id": "",
      "title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering",
      "authors": "Federico Cocchi, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cocchi_Augmenting_Multimodal_LLMs_with_Self-Reflective_Tokens_for_Knowledge-based_Visual_Question_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1021",
      "paper_id": "",
      "title": "AutoSSVH: Exploring Automated Frame Sampling for Efficient Self-Supervised Video Hashing",
      "authors": "Niu Lian, Jun Li, Jinpeng Wang, Ruisheng Luo, Yaowei Wang, Shu-Tao Xia, Bin Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lian_AutoSSVH_Exploring_Automated_Frame_Sampling_for_Efficient_Self-Supervised_Video_Hashing_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_598",
      "paper_id": "",
      "title": "Bayesian Exploration of Pre-trained Models for Low-shot Image Classification",
      "authors": "Yibo Miao, Yu Lei, Feng Zhou, Zhijie Deng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Bayesian_Exploration_of_Pre-trained_Models_for_Low-shot_Image_Classification_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_868",
      "paper_id": "",
      "title": "Bayesian Test-Time Adaptation for Vision-Language Models",
      "authors": "Lihua Zhou, Mao Ye, Shuaifeng Li, Nianxin Li, Xiatian Zhu, Lei Deng, Hongbin Liu, Zhen Lei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Bayesian_Test-Time_Adaptation_for_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_433",
      "paper_id": "",
      "title": "Beyond Image Classification: A Video Benchmark and Dual-Branch Hybrid Discrimination Framework for Compositional Zero-Shot Learning",
      "authors": "Dongyao Jiang, Haodong Jing, Yongqiang Ma, Nanning Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Beyond_Image_Classification_A_Video_Benchmark_and_Dual-Branch_Hybrid_Discrimination_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1723",
      "paper_id": "",
      "title": "Beyond Seen Primitive Concepts and Attribute-Object Compositional Learning",
      "authors": "Nirat Saini, Khoi Pham, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Saini_Beyond_Seen_Primitive_Concepts_and_Attribute-Object_Compositional_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2108",
      "paper_id": "",
      "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning",
      "authors": "Hairui Ren, Fan Tang, He Zhao, Zixuan Wang, Dandan Guo, Yi Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_Beyond_Words_Augmenting_Discriminative_Richness_via_Diffusions_in_Unsupervised_Prompt_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1094",
      "paper_id": "",
      "title": "Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning",
      "authors": "Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-Directional_Distribution_Alignment_for_Transductive_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_373",
      "paper_id": "",
      "title": "BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency",
      "authors": "Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, Min Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BiCro_Noisy_Correspondence_Rectification_for_Multi-Modality_Data_via_Bi-Directional_Cross-Modal_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1596",
      "paper_id": "",
      "title": "BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning",
      "authors": "Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, Kyungwoo Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Oh_BlackVIP_Black-Box_Visual_Prompting_for_Robust_Transfer_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_271",
      "paper_id": "",
      "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
      "authors": "Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, Yi Zeng, Lei Wu, Liuyang Bian, Zhaoxiong Wang, Long Liu, Yanzhou Yang, Han Xiao, Aojun Zhou, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_BlueLM-V-3B_Algorithm_and_System_Co-Design_for_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1466",
      "paper_id": "",
      "title": "Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy",
      "authors": "Zesen Cheng, Hang Zhang, Kehan Li, Sicong Leng, Zhiqiang Hu, Fei Wu, Deli Zhao, Xin Li, Lidong Bing",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1859",
      "paper_id": "",
      "title": "Bridging Modalities: Improving Universal Multimodal Retrieval by Multimodal Large Language Models",
      "authors": "Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2440",
      "paper_id": "",
      "title": "Building Vision-Language Models on Solid Foundations with Masked Distillation",
      "authors": "Sepehr Sameni, Kushal Kafle, Hao Tan, Simon Jenni",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sameni_Building_Vision-Language_Models_on_Solid_Foundations_with_Masked_Distillation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2199",
      "paper_id": "",
      "title": "Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations",
      "authors": "Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence Staib, James S. Duncan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/You_Calibrating_Multi-modal_Representations_A_Pursuit_of_Group_Robustness_without_Annotations_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1941",
      "paper_id": "",
      "title": "CapDet: Unifying Dense Captioning and Open-World Detection Pretraining",
      "authors": "Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, Xiaodan Liang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Long_CapDet_Unifying_Dense_Captioning_and_Open-World_Detection_Pretraining_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1705",
      "paper_id": "",
      "title": "CapsFusion: Rethinking Image-Text Data at Scale",
      "authors": "Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, Jingjing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_CapsFusion_Rethinking_Image-Text_Data_at_Scale_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2132",
      "paper_id": "",
      "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
      "authors": "Mohsen Gholami, Mohammad Akbari, Kevin Cannons, Yong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2628",
      "paper_id": "",
      "title": "CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval",
      "authors": "Likai Tian, Jian Zhao, Zechao Hu, Zhengwei Yang, Hao Li, Lei Jin, Zheng Wang, Xuelong Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2082",
      "paper_id": "",
      "title": "Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval",
      "authors": "Zhen-Duo Chen, Li-Jun Zhao, Zi-Chao Zhang, Xin Luo, Xin-Shun Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Characteristics_Matching_Based_Hash_Codes_Generation_for_Efficient_Fine-grained_Image_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1852",
      "paper_id": "",
      "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
      "authors": "Dongseob Kim, Hyunjung Shim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Classifier-guided_CLIP_Distillation_for_Unsupervised_Multi-label_Classification_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_172",
      "paper_id": "",
      "title": "CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not",
      "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1547",
      "paper_id": "",
      "title": "CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation",
      "authors": "Reza Abbasi, Ali Nazari, Aminreza Sefid, Mohammadali Banayeeanzade, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_2003",
      "paper_id": "",
      "title": "CLIP-Event: Connecting Text and Images With Event Structures",
      "authors": "Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, Shih-Fu Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1636",
      "paper_id": "",
      "title": "CLIP-KD: An Empirical Study of CLIP Model Distillation",
      "authors": "Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, Yongjun Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2003",
      "paper_id": "",
      "title": "CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval",
      "authors": "Renjing Pei, Jianzhuang Liu, Weimian Li, Bin Shao, Songcen Xu, Peng Dai, Juwei Lu, Youliang Yan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1912",
      "paper_id": "",
      "title": "CLIPPO: Image-and-Language Understanding From Pixels Only",
      "authors": "Michael Tschannen, Basil Mustafa, Neil Houlsby",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2131",
      "paper_id": "",
      "title": "Cloud-Device Collaborative Learning for Multimodal Large Language Models",
      "authors": "Guanqun Wang, Jiaming Liu, Chenxuan Li, Yuan Zhang, Junpeng Ma, Xinyu Wei, Kevin Zhang, Maurice Chong, Renrui Zhang, Yijiang Liu, Shanghang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Cloud-Device_Collaborative_Learning_for_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2370",
      "paper_id": "",
      "title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection",
      "authors": "Jinqi Xiao, Shen Sang, Tiancheng Zhi, Jing Liu, Qing Yan, Linjie Luo, Bo Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_COAP_Memory-Efficient_Training_with_Correlation-Aware_Gradient_Projection_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_541",
      "paper_id": "",
      "title": "COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Adaptation",
      "authors": "Arnav M. Das, Gantavya Bhatt, Lilly Kumari, Sahil Verma, Jeff Bilmes",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Das_COBRA_COmBinatorial_Retrieval_Augmentation_for_Few-Shot_Adaptation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2691",
      "paper_id": "",
      "title": "Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models",
      "authors": "Zichen Miao, Wei Chen, Qiang Qiu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1152",
      "paper_id": "",
      "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
      "authors": "Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul Chilimbi, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huynh_CoLLM_A_Large_Language_Model_for_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2009",
      "paper_id": "",
      "title": "Composed Video Retrieval via Enriched Context and Discriminative Embeddings",
      "authors": "Omkar Thawakar, Muzammal Naseer, Rao Muhammad Anwer, Salman Khan, Michael Felsberg, Mubarak Shah, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Thawakar_Composed_Video_Retrieval_via_Enriched_Context_and_Discriminative_Embeddings_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_461",
      "paper_id": "",
      "title": "Composing Object Relations and Attributes for Image-Text Matching",
      "authors": "Khoi Pham, Chuong Huynh, Ser-Nam Lim, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Pham_Composing_Object_Relations_and_Attributes_for_Image-Text_Matching_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1150",
      "paper_id": "",
      "title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection",
      "authors": "Marco Garosi, Alessandro Conti, Gaowen Liu, Elisa Ricci, Massimiliano Mancini",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_757",
      "paper_id": "",
      "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
      "authors": "Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_662",
      "paper_id": "",
      "title": "ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks",
      "authors": "Andrea Rosasco, Stefano Berti, Giulia Pasquale, Damiano Malafronte, Shogo Sato, Hiroyuki Segawa, Tetsugo Inada, Lorenzo Natale",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Rosasco_ConCon-Chi_Concept-Context_Chimera_Benchmark_for_Personalized_Vision-Language_Tasks_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1042",
      "paper_id": "",
      "title": "Conditional Prompt Learning for Vision-Language Models",
      "authors": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_702",
      "paper_id": "",
      "title": "Conformal Prediction for Zero-Shot Models",
      "authors": "Julio Silva-Rodríguez, Ismail Ben Ayed, Jose Dolz",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Silva-Rodriguez_Conformal_Prediction_for_Zero-Shot_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1230",
      "paper_id": "",
      "title": "Conical Visual Concentration for Efficient Large Vision-Language Models",
      "authors": "Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xing_Conical_Visual_Concentration_for_Efficient_Large_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1755",
      "paper_id": "",
      "title": "Context-Aware Multimodal Pretraining",
      "authors": "Karsten Roth, Zeynep Akata, Dima Damen, Ivana Balazevic, Olivier J. Henaff",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Roth_Context-Aware_Multimodal_Pretraining_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2058",
      "paper_id": "",
      "title": "Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning",
      "authors": "Yun Li, Zhe Liu, Hang Chen, Lina Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Context-based_and_Diversity-driven_Specificity_in_Compositional_Zero-Shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1541",
      "paper_id": "",
      "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval",
      "authors": "Eric Xing, Pranavi Kolouju, Robert Pless, Abby Stylianou, Nathan Jacobs",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xing_ConText-CIR_Learning_from_Concepts_in_Text_for_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1606",
      "paper_id": "",
      "title": "ContextSeg: Sketch Semantic Segmentation by Querying the Context with Attention",
      "authors": "Jiawei Wang, Changjian Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_ContextSeg_Sketch_Semantic_Segmentation_by_Querying_the_Context_with_Attention_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1829",
      "paper_id": "",
      "title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding",
      "authors": "Le Zhang, Rabiul Awal, Aishwarya Agrawal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Contrasting_Intra-Modal_and_Ranking_Cross-Modal_Hard_Negatives_to_Enhance_Visio-Linguistic_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1234",
      "paper_id": "",
      "title": "Contrastive Embedding for Generalized Zero-Shot Learning",
      "authors": "Zongyan Han, Zhenyong Fu, Shuo Chen, Jian Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Han_Contrastive_Embedding_for_Generalized_Zero-Shot_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1031",
      "paper_id": "",
      "title": "ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing",
      "authors": "Zequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang, Bo Chen, Zhengjue Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_709",
      "paper_id": "",
      "title": "CORA: Adapting CLIP for Open-Vocabulary Detection With Region Prompting and Anchor Pre-Matching",
      "authors": "Xiaoshi Wu, Feng Zhu, Rui Zhao, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_589",
      "paper_id": "",
      "title": "Correlation Verification for Image Retrieval",
      "authors": "Seongwon Lee, Hongje Seong, Suhyeon Lee, Euntai Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Correlation_Verification_for_Image_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1554",
      "paper_id": "",
      "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning",
      "authors": "Lei-Lei Ma, Shuo Xu, Ming-Kun Xie, Lei Wang, Dengdi Sun, Haifeng Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_Correlative_and_Discriminative_Label_Grouping_for_Multi-Label_Visual_Prompt_Tuning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_603",
      "paper_id": "",
      "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
      "authors": "Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, Zhi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_COSMIC_Clique-Oriented_Semantic_Multi-space_Integration_for_Robust_CLIP_Test-Time_Adaptation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_465",
      "paper_id": "",
      "title": "CoSMo: Content-Style Modulation for Image Retrieval With Text Feedback",
      "authors": "Seungmin Lee, Dongwan Kim, Bohyung Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_CoSMo_Content-Style_Modulation_for_Image_Retrieval_With_Text_Feedback_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1068",
      "paper_id": "",
      "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training",
      "authors": "Sanghwan Kim, Rui Xiao, Mariana-Iuliana Georgescu, Stephan Alaniz, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_COSMOS_Cross-Modality_Self-Distillation_for_Vision_Language_Pre-training_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_9",
      "paper_id": "",
      "title": "Counterfactual Zero-Shot and Open-Set Visual Recognition",
      "authors": "Zhongqi Yue, Tan Wang, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yue_Counterfactual_Zero-Shot_and_Open-Set_Visual_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1275",
      "paper_id": "",
      "title": "Critical Learning Periods for Multisensory Integration in Deep Networks",
      "authors": "Michael Kleinman, Alessandro Achille, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kleinman_Critical_Learning_Periods_for_Multisensory_Integration_in_Deep_Networks_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2463",
      "paper_id": "",
      "title": "Cropper: Vision-Language Model for Image Cropping through In-Context Learning",
      "authors": "Seung Hyun Lee, Jijun Jiang, Yiran Xu, Zhuofang Li, Junjie Ke, Yinxiao Li, Junfeng He, Steven Hickson, Katie Datsenko, Sangpil Kim, Ming-Hsuan Yang, Irfan Essa, Feng Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Cropper_Vision-Language_Model_for_Image_Cropping_through_In-Context_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_402",
      "paper_id": "",
      "title": "Cross Modal Retrieval With Querybank Normalisation",
      "authors": "Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, Samuel Albanie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bogolin_Cross_Modal_Retrieval_With_Querybank_Normalisation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_136",
      "paper_id": "",
      "title": "Cross-Domain Image Captioning With Discriminative Finetuning",
      "authors": "Roberto Dessì, Michele Bevilacqua, Eleonora Gualdoni, Nathanaël Carraz Rakotonirina, Francesca Franzon, Marco Baroni",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dessi_Cross-Domain_Image_Captioning_With_Discriminative_Finetuning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_888",
      "paper_id": "",
      "title": "Cross-Modal Center Loss for 3D Cross-Modal Retrieval",
      "authors": "Longlong Jing, Elahe Vahdani, Jiaxing Tan, Yingli Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2543",
      "paper_id": "",
      "title": "CustomKD: Customizing Large Vision Foundation for Edge Model Improvement via Knowledge Distillation",
      "authors": "Jungsoo Lee, Debasmit Das, Munawar Hayat, Sungha Choi, Kyuwoong Hwang, Fatih Porikli",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_CustomKD_Customizing_Large_Vision_Foundation_for_Edge_Model_Improvement_via_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_267",
      "paper_id": "",
      "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
      "authors": "Li Ren, Chen Chen, Liqiang Wang, Kien Hua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_DA-VPT_Semantic-Guided_Visual_Prompt_Tuning_for_Vision_Transformers_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_848",
      "paper_id": "",
      "title": "Data-Efficient Multimodal Fusion on a Single GPU",
      "authors": "Noël Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Vouitsis_Data-Efficient_Multimodal_Fusion_on_a_Single_GPU_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_303",
      "paper_id": "",
      "title": "Data-Free Sketch-Based Image Retrieval",
      "authors": "Abhra Chaudhuri, Ayan Kumar Bhunia, Yi-Zhe Song, Anjan Dutta",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chaudhuri_Data-Free_Sketch-Based_Image_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_457",
      "paper_id": "",
      "title": "DATE: Domain Adaptive Product Seeker for E-Commerce",
      "authors": "Haoyuan Li, Hao Jiang, Tao Jin, Mengyan Li, Yan Chen, Zhijie Lin, Yang Zhao, Zhou Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DATE_Domain_Adaptive_Product_Seeker_for_E-Commerce_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2330",
      "paper_id": "",
      "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
      "authors": "Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DeCLIP_Decoupled_Learning_for_Open-Vocabulary_Dense_Perception_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1888",
      "paper_id": "",
      "title": "Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning",
      "authors": "Xiaocheng Lu, Song Guo, Ziming Liu, Jingcai Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Decomposed_Soft_Prompt_Guided_Fusion_Enhancing_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_995",
      "paper_id": "",
      "title": "Deep Hashing With Minimal-Distance-Separated Hash Centers",
      "authors": "Liangdao Wang, Yan Pan, Cong Liu, Hanjiang Lai, Jian Yin, Ye Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1034",
      "paper_id": "",
      "title": "Defending Multimodal Fusion Models Against Single-Source Adversaries",
      "authors": "Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, Zico Kolter",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2605",
      "paper_id": "",
      "title": "DeIL: Direct-and-Inverse CLIP for Open-World Few-Shot Learning",
      "authors": "Shuai Shao, Yu Bai, Yan Wang, Baodi Liu, Yicong Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_DeIL_Direct-and-Inverse_CLIP_for_Open-World_Few-Shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1582",
      "paper_id": "",
      "title": "DenseCLIP: Language-Guided Dense Prediction With Context-Aware Prompting",
      "authors": "Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1996",
      "paper_id": "",
      "title": "DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models",
      "authors": "Yongqi Huang, Peng Ye, Chenyu Huang, Jianjian Cao, Lin Zhang, Baopu Li, Gang Yu, Tao Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_DeRS_Towards_Extremely_Efficient_Upcycled_Mixture-of-Experts_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1607",
      "paper_id": "",
      "title": "Describing Differences in Image Sets with Natural Language",
      "authors": "Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Dunlap_Describing_Differences_in_Image_Sets_with_Natural_Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2034",
      "paper_id": "",
      "title": "Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning",
      "authors": "Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liao_Descriptor_and_Word_Soups_Overcoming_the_Parameter_Efficiency_Accuracy_Tradeoff_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1175",
      "paper_id": "",
      "title": "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment",
      "authors": "Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Hang Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_913",
      "paper_id": "",
      "title": "DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection",
      "authors": "Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, Dan Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_DetCLIPv3_Towards_Versatile_Generative_Open-vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_785",
      "paper_id": "",
      "title": "Detecting Everything in the Open World: Towards Universal Object Detection",
      "authors": "Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, Shengjin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2210",
      "paper_id": "",
      "title": "Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding",
      "authors": "Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_Detection_Hub_Unifying_Object_Detection_Datasets_via_Query_Adaptation_on_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2129",
      "paper_id": "",
      "title": "DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning",
      "authors": "Kun Zhang, Jingyu Li, Zhe Li, S.Kevin Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_DH-Set_Improving_Vision-Language_Alignment_with_Diverse_and_Hybrid_Set-Embeddings_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2536",
      "paper_id": "",
      "title": "Diffusion Bridge: Leveraging Diffusion Model to Reduce the Modality Gap Between Text and Vision for Zero-Shot Image Captioning",
      "authors": "Jeong Ryong Lee, Yejee Shin, Geonhui Son, Dosik Hwang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Diffusion_Bridge_Leveraging_Diffusion_Model_to_Reduce_the_Modality_Gap_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1376",
      "paper_id": "",
      "title": "DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment",
      "authors": "Cijo Jose, Théo Moutakanni, Dahyun Kang, Federico Baldassarre, Timothée Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Michaël Ramamonjisoa, Maxime Oquab, Oriane Siméoni, Huy V. Vo, Patrick Labatut, Piotr Bojanowski",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jose_DINOv2_Meets_Text_A_Unified_Framework_for_Image-_and_Pixel-Level_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_462",
      "paper_id": "",
      "title": "DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training",
      "authors": "Yihao Chen, Xianbiao Qi, Jianan Wang, Lei Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DisCo-CLIP_A_Distributed_Contrastive_Loss_for_Memory_Efficient_CLIP_Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2530",
      "paper_id": "",
      "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning",
      "authors": "Xueyi Ke, Satoshi Tsutsui, Yayun Zhang, Bihan Wen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ke_Discovering_Hidden_Visual_Concepts_Beyond_Linguistic_Input_in_Infant_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_780",
      "paper_id": "",
      "title": "Discrete-Continuous Action Space Policy Gradient-Based Attention for Image-Text Matching",
      "authors": "Shiyang Yan, Li Yu, Yuan Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Discrete-Continuous_Action_Space_Policy_Gradient-Based_Attention_for_Image-Text_Matching_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_366",
      "paper_id": "",
      "title": "Disentangled Representation Learning for Unsupervised Neural Quantization",
      "authors": "Haechan Noh, Sangeek Hyun, Woojin Jeong, Hanshin Lim, Jae-Pil Heo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1842",
      "paper_id": "",
      "title": "Disentangling Visual and Written Concepts in CLIP",
      "authors": "Joanna Materzyńska, Antonio Torralba, David Bau",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Materzynska_Disentangling_Visual_and_Written_Concepts_in_CLIP_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1348",
      "paper_id": "",
      "title": "Disentangling Visual Embeddings for Attributes and Objects",
      "authors": "Nirat Saini, Khoi Pham, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Saini_Disentangling_Visual_Embeddings_for_Attributes_and_Objects_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_599",
      "paper_id": "",
      "title": "Distilling Audio-Visual Knowledge by Compositional Contrastive Learning",
      "authors": "Yanbei Chen, Yongqin Xian, A. Sophia Koepke, Ying Shan, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Distilling_Audio-Visual_Knowledge_by_Compositional_Contrastive_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_260",
      "paper_id": "",
      "title": "Distinguishing Unseen From Seen for Generalized Zero-Shot Learning",
      "authors": "Hongzu Su, Jingjing Li, Zhi Chen, Lei Zhu, Ke Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Su_Distinguishing_Unseen_From_Seen_for_Generalized_Zero-Shot_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_233",
      "paper_id": "",
      "title": "DiTASK: Multi-Task Fine-Tuning with Diffeomorphic Transformations",
      "authors": "Krishna Sri Ipsit Mantri, Carola-Bibiane Schönlieb, Bruno Ribeiro, Chaim Baskin, Moshe Eliasof",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mantri_DiTASK_Multi-Task_Fine-Tuning_with_Diffeomorphic_Transformations_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_487",
      "paper_id": "",
      "title": "Diversity-Aware Meta Visual Prompting",
      "authors": "Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_196",
      "paper_id": "",
      "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
      "authors": "Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Alvar_DivPrune_Diversity-based_Visual_Token_Pruning_for_Large_Multimodal_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1968",
      "paper_id": "",
      "title": "Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?",
      "authors": "Yancheng Cai, Fei Yin, Dounia Hammou, Rafal Mantiuk",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1233",
      "paper_id": "",
      "title": "Do Vision and Language Encoders Represent the World Similarly?",
      "authors": "Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, Noel E. O'Connor",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Maniparambil_Do_Vision_and_Language_Encoders_Represent_the_World_Similarly_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_215",
      "paper_id": "",
      "title": "Domain Prompt Learning with Quaternion Networks",
      "authors": "Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_Domain_Prompt_Learning_with_Quaternion_Networks_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_741",
      "paper_id": "",
      "title": "Doubly Right Object Recognition: A Why Prompt for Visual Rationales",
      "authors": "Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, Carl Vondrick",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Mao_Doubly_Right_Object_Recognition_A_Why_Prompt_for_Visual_Rationales_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1053",
      "paper_id": "",
      "title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models",
      "authors": "Haoyang Li, Liang Wang, Chao Wang, Jing Jiang, Yan Peng, Guodong Long",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DPC_Dual-Prompt_Collaboration_for_Tuning_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1455",
      "paper_id": "",
      "title": "Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval",
      "authors": "Xiaoshuai Hao, Wanqian Zhang, Dayan Wu, Fei Zhu, Bo Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_249",
      "paper_id": "",
      "title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models",
      "authors": "Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2139",
      "paper_id": "",
      "title": "DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers",
      "authors": "Mert Bülent Sarıyıldız, Philippe Weinzaepfel, Thomas Lucas, Pau de Jorge, Diane Larlus, Yannis Kalantidis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Sariyildiz_DUNE_Distilling_a_Universal_Encoder_from_Heterogeneous_2D_and_3D_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1107",
      "paper_id": "",
      "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models",
      "authors": "Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tao_DyCoke_Dynamic_Compression_of_Tokens_for_Fast_Video_Large_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_201",
      "paper_id": "",
      "title": "Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis",
      "authors": "Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Dynamic_Adapter_Meets_Prompt_Tuning_Parameter-Efficient_Transfer_Learning_for_Point_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_246",
      "paper_id": "",
      "title": "Dynamic Inference With Grounding Based Vision and Language Models",
      "authors": "Burak Uzkent, Amanmeet Garg, Wentao Zhu, Keval Doshi, Jingru Yi, Xiaolong Wang, Mohamed Omar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1350",
      "paper_id": "",
      "title": "Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features",
      "authors": "Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_781",
      "paper_id": "",
      "title": "Efficient Multimodal Fusion via Interactive Prompting",
      "authors": "Yaowei Li, Ruijie Quan, Linchao Zhu, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1756",
      "paper_id": "",
      "title": "Efficient Stitchable Task Adaptation",
      "authors": "Haoyu He, Zizheng Pan, Jing Liu, Jianfei Cai, Bohan Zhuang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/He_Efficient_Stitchable_Task_Adaptation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_942",
      "paper_id": "",
      "title": "Efficient Test-Time Adaptation of Vision-Language Models",
      "authors": "Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric Xing",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2086",
      "paper_id": "",
      "title": "Efficient Vision-Language Pre-training by Cluster Masking",
      "authors": "Zihao Wei, Zixuan Pan, Andrew Owens",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Efficient_Vision-Language_Pre-training_by_Cluster_Masking_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2569",
      "paper_id": "",
      "title": "EfficientLLaVA: Generalizable Auto-Pruning for Large Vision-language Models",
      "authors": "Yinan Liang, Ziwei Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_EfficientLLaVA_Generalizable_Auto-Pruning_for_Large_Vision-language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_559",
      "paper_id": "",
      "title": "EI-CLIP: Entity-Aware Interventional Contrastive Learning for E-Commerce Cross-Modal Retrieval",
      "authors": "Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale, Zhangyang Wang, Tong Yu, Jiuxiang Gu, Sunav Choudhary, Xiaohui Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_EI-CLIP_Entity-Aware_Interventional_Contrastive_Learning_for_E-Commerce_Cross-Modal_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1502",
      "paper_id": "",
      "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning",
      "authors": "Jinpeng Wang, Tianci Luo, Yaohua Zha, Yan Feng, Ruisheng Luo, Bin Chen, Tao Dai, Long Chen, Yaowei Wang, Shu-Tao Xia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Embracing_Collaboration_Over_Competition_Condensing_Multiple_Prompts_for_Visual_In-Context_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1459",
      "paper_id": "",
      "title": "Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion",
      "authors": "Zixian Gao, Xun Jiang, Xing Xu, Fumin Shen, Yujie Li, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_716",
      "paper_id": "",
      "title": "En-Compactness: Self-Distillation Embedding & Contrastive Generation for Generalized Zero-Shot Learning",
      "authors": "Xia Kong, Zuodong Gao, Xiaofan Li, Ming Hong, Jun Liu, Chengjie Wang, Yuan Xie, Yanyun Qu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kong_En-Compactness_Self-Distillation_Embedding__Contrastive_Generation_for_Generalized_Zero-Shot_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2264",
      "paper_id": "",
      "title": "Enhanced Multimodal Representation Learning With Cross-Modal KD",
      "authors": "Mengxi Chen, Linyu Xing, Yu Wang, Ya Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2529",
      "paper_id": "",
      "title": "Enhancing Multimodal Cooperation via Sample-level Modality Valuation",
      "authors": "Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Enhancing_Multimodal_Cooperation_via_Sample-level_Modality_Valuation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2847",
      "paper_id": "",
      "title": "Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data",
      "authors": "Haoxin Li, Boyang Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Enhancing_Vision-Language_Compositional_Understanding_with_Multimodal_Synthetic_Data_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_497",
      "paper_id": "",
      "title": "Enhancing Vision-Language Pre-training with Rich Supervisions",
      "authors": "Yuan Gao, Kunyu Shi, Pengkai Zhu, Edouard Belval, Oren Nuriel, Srikar Appalaraju, Shabnam Ghadar, Zhuowen Tu, Vijay Mahadevan, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Enhancing_Vision-Language_Pre-training_with_Rich_Supervisions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2736",
      "paper_id": "",
      "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces",
      "authors": "Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodolà, Simone Melzi, Maks Ovsjanikov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hadgi_Escaping_Platos_Cave_Towards_the_Alignment_of_3D_and_Text_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_942",
      "paper_id": "",
      "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
      "authors": "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2321",
      "paper_id": "",
      "title": "EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension",
      "authors": "Jiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, Hideki Nakayama",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_EVCap_Retrieval-Augmented_Image_Captioning_with_External_Visual-Name_Memory_for_Open-World_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1064",
      "paper_id": "",
      "title": "EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata",
      "authors": "Chenhao Zheng, Ayush Shrivastava, Andrew Owens",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_EXIF_As_Language_Learning_Cross-Modal_Associations_Between_Images_and_Camera_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1895",
      "paper_id": "",
      "title": "Expanding Large Pre-Trained Unimodal Models With Multimodal Information Injection for Image-Text Multimodal Classification",
      "authors": "Tao Liang, Guosheng Lin, Mingyang Wan, Tianrui Li, Guojun Ma, Fengmao Lv",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expanding_Large_Pre-Trained_Unimodal_Models_With_Multimodal_Information_Injection_for_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1497",
      "paper_id": "",
      "title": "Explaining CLIP's Performance Disparities on Data from Blind/Low Vision Users",
      "authors": "Daniela Massiceti, Camilla Longden, Agnieszka Slowik, Samuel Wills, Martin Grayson, Cecily Morrison",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Massiceti_Explaining_CLIPs_Performance_Disparities_on_Data_from_BlindLow_Vision_Users_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2243",
      "paper_id": "",
      "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
      "authors": "Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_107",
      "paper_id": "",
      "title": "Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR",
      "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Soumitri Chattopadhyay, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sain_Exploiting_Unlabelled_Photos_for_Stronger_Fine-Grained_SBIR_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_573",
      "paper_id": "",
      "title": "Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection",
      "authors": "Heng Zhang, Qiuyu Zhao, Linyu Zheng, Hao Zeng, Zhiwei Ge, Tianhao Li, Sulong Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Region-Word_Alignment_in_Built-in_Detector_for_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2420",
      "paper_id": "",
      "title": "Exploring Regional Clues in CLIP for Zero-Shot Semantic Segmentation",
      "authors": "Yi Zhang, Meng-Hao Guo, Miao Wang, Shi-Min Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Regional_Clues_in_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_45",
      "paper_id": "",
      "title": "Exploring Structured Semantic Prior for Multi Label Recognition With Incomplete Labels",
      "authors": "Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, Jungong Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2336",
      "paper_id": "",
      "title": "Exploring the Transferability of Visual Prompting for Multimodal Large Language Models",
      "authors": "Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, Jun Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_the_Transferability_of_Visual_Prompting_for_Multimodal_Large_Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1358",
      "paper_id": "",
      "title": "F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
      "authors": "Pramit Saha, Felix Wagner, Divyanshu Mishra, Can Peng, Anshul Thakur, David A. Clifton, Konstantinos Kamnitsas, J. Alison Noble",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2664",
      "paper_id": "",
      "title": "Fair-VPT: Fair Visual Prompt Tuning for Image Classification",
      "authors": "Sungho Park, Hyeran Byun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Park_Fair-VPT_Fair_Visual_Prompt_Tuning_for_Image_Classification_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1663",
      "paper_id": "",
      "title": "Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones",
      "authors": "Mert Bülent Sarıyıldız, Karteek Alahari, Diane Larlus, Yannis Kalantidis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1286",
      "paper_id": "",
      "title": "FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks",
      "authors": "Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe Song, Tao Xiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_837",
      "paper_id": "",
      "title": "Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback",
      "authors": "Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, Rogerio Feris",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Fashion_IQ_A_New_Dataset_Towards_Retrieving_Images_by_Natural_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1169",
      "paper_id": "",
      "title": "FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training",
      "authors": "Yunpeng Han, Lisai Zhang, Qingcai Chen, Zhijian Chen, Zhonghua Li, Jianxin Yang, Zhao Cao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FashionSAP_Symbols_and_Attributes_Prompt_for_Fine-Grained_Fashion_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1670",
      "paper_id": "",
      "title": "FashionVLP: Vision Language Transformer for Fashion Retrieval With Feedback",
      "authors": "Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau, Pradeep Natarajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Goenka_FashionVLP_Vision_Language_Transformer_for_Fashion_Retrieval_With_Feedback_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1761",
      "paper_id": "",
      "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
      "authors": "Kwonyoung Kim, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Faster_Parameter-Efficient_Tuning_with_Token_Redundancy_Reduction_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1957",
      "paper_id": "",
      "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
      "authors": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Vasu_FastVLM_Efficient_Vision_Encoding_for_Vision_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2116",
      "paper_id": "",
      "title": "Few-shot Learner Parameterization by Diffusion Time-steps",
      "authors": "Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, Qianru Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_Few-shot_Learner_Parameterization_by_Diffusion_Time-steps_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2220",
      "paper_id": "",
      "title": "Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment",
      "authors": "Runqi Wang, Hao Zheng, Xiaoyue Duan, Jianzhuang Liu, Yuning Lu, Tian Wang, Songcen Xu, Baochang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_778",
      "paper_id": "",
      "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
      "authors": "Tian Liu, Huixin Zhang, Shubham Parashar, Shu Kong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Few-Shot_Recognition_via_Stage-Wise_Retrieval-Augmented_Finetuning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1091",
      "paper_id": "",
      "title": "FFF: Fixing Flawed Foundations in Contrastive Pre-Training Results in Very Strong Vision-Language Models",
      "authors": "Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bulat_FFF_Fixing_Flawed_Foundations_in_Contrastive_Pre-Training_Results_in_Very_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1194",
      "paper_id": "",
      "title": "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training",
      "authors": "Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, Dhruv Mahajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_522",
      "paper_id": "",
      "title": "Finding Badly Drawn Bunnies",
      "authors": "Lan Yang, Kaiyue Pang, Honggang Zhang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Finding_Badly_Drawn_Bunnies_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1141",
      "paper_id": "",
      "title": "Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network",
      "authors": "Zhengxin Pan, Fangyu Wu, Bailing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1982",
      "paper_id": "",
      "title": "Fine-grained Prototypical Voting with Heterogeneous Mixup for Semi-supervised 2D-3D Cross-modal Retrieval",
      "authors": "Fan Zhang, Xian-Sheng Hua, Chong Chen, Xiao Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Fine-grained_Prototypical_Voting_with_Heterogeneous_Mixup_for_Semi-supervised_2D-3D_Cross-modal_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1652",
      "paper_id": "",
      "title": "Fine-Tuning Image Transformers Using Learnable Memory",
      "authors": "Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, Andrew Jackson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sandler_Fine-Tuning_Image_Transformers_Using_Learnable_Memory_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1336",
      "paper_id": "",
      "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs",
      "authors": "Mothilal Asokan, Kebin Wu, Fatima Albreiki",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Asokan_FineLIP_Extending_CLIPs_Reach_via_Fine-Grained_Alignment_with_Longer_Text_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2222",
      "paper_id": "",
      "title": "Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models",
      "authors": "Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, Aditi Raghunathan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1987",
      "paper_id": "",
      "title": "FLAIR: VLM with Fine-grained Language-informed Image Representations",
      "authors": "Rui Xiao, Sanghwan Kim, Mariana-Iuliana Georgescu, Zeynep Akata, Stephan Alaniz",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_FLAIR_VLM_with_Fine-grained_Language-informed_Image_Representations_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1971",
      "paper_id": "",
      "title": "FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training",
      "authors": "Anjia Cao, Xing Wei, Zhiheng Ma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cao_FLAME_Frozen_Large_Language_Models_Enable_Data-Efficient_Language-Image_Pre-training_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1202",
      "paper_id": "",
      "title": "FlashSloth : Lightning Multimodal Large Language Models via Embedded Visual Compression",
      "authors": "Bo Tong, Bokai Lai, Yiyi Zhou, Gen Luo, Yunhang Shen, Ke Li, Xiaoshuai Sun, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tong_FlashSloth__Lightning_Multimodal_Large_Language_Models_via_Embedded_Visual_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_676",
      "paper_id": "",
      "title": "FLAVA: A Foundational Language and Vision Alignment Model",
      "authors": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2301",
      "paper_id": "",
      "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
      "authors": "Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_37",
      "paper_id": "",
      "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM",
      "authors": "Qiyuan Dai, Sibei Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Dai_Free_on_the_Fly_Enhancing_Flexibility_in_Test-Time_Adaptation_with_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_775",
      "paper_id": "",
      "title": "Fuzzy Multimodal Learning for Trusted Cross-modal Retrieval",
      "authors": "Siyuan Duan, Yuan Sun, Dezhong Peng, Zheng Liu, Xiaomin Song, Peng Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Fuzzy_Multimodal_Learning_for_Trusted_Cross-modal_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_667",
      "paper_id": "",
      "title": "General Object Foundation Model for Images and Videos at Scale",
      "authors": "Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_109",
      "paper_id": "",
      "title": "Generalized Zero-Shot Classification via Semantics-Free Inter-Class Feature Generation",
      "authors": "Libiao Chen, Dong Nie, Junjun Pan, Jing Yan, Zhenyu Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Generalized_Zero-Shot_Classification_via_Semantics-Free_Inter-Class_Feature_Generation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_846",
      "paper_id": "",
      "title": "Generating Enhanced Negatives for Training Language-Based Object Detectors",
      "authors": "Shiyu Zhao, Long Zhao, Vijay Kumar B G, Yumin Suh, Dimitris N. Metaxas, Manmohan Chandraker, Samuel Schulter",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Generating_Enhanced_Negatives_for_Training_Language-Based_Object_Detectors_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2215",
      "paper_id": "",
      "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
      "authors": "JungKyoo Shin, Bumsoo Kim, Eunwoo Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2472",
      "paper_id": "",
      "title": "Generative Region-Language Pretraining for Open-Ended Object Detection",
      "authors": "Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Generative_Region-Language_Pretraining_for_Open-Ended_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2836",
      "paper_id": "",
      "title": "Generative Zero-Shot Composed Image Retrieval",
      "authors": "Lan Wang, Wei Ao, Vishnu Naresh Boddeti, Ser-Nam Lim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Generative_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_874",
      "paper_id": "",
      "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
      "authors": "Sungyeon Kim, Xinliang Zhu, Xiaofan Lin, Muhammet Bastan, Douglas Gray, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_GENIUS_A_Generative_Framework_for_Universal_Multimodal_Search_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1716",
      "paper_id": "",
      "title": "GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods",
      "authors": "Da Yin, Feng Gao, Govind Thattai, Michael Johnston, Kai-Wei Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_GIVL_Improving_Geographical_Inclusivity_of_Vision-Language_Models_With_Pre-Training_Methods_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1099",
      "paper_id": "",
      "title": "GLID: Pre-training a Generalist Encoder-Decoder Vision Model",
      "authors": "Jihao Liu, Jinliang Zheng, Yu Liu, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GLID_Pre-training_a_Generalist_Encoder-Decoder_Vision_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1288",
      "paper_id": "",
      "title": "GlideNet: Global, Local and Intrinsic Based Dense Embedding NETwork for Multi-Category Attributes Prediction",
      "authors": "Kareem Metwaly, Aerin Kim, Elliot Branson, Vishal Monga",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Metwaly_GlideNet_Global_Local_and_Intrinsic_Based_Dense_Embedding_NETwork_for_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_2067",
      "paper_id": "",
      "title": "Globetrotter: Connecting Languages by Connecting Images",
      "authors": "Dídac Surís, Dave Epstein, Carl Vondrick",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Suris_Globetrotter_Connecting_Languages_by_Connecting_Images_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1315",
      "paper_id": "",
      "title": "Goal-Oriented Gaze Estimation for Zero-Shot Learning",
      "authors": "Yang Liu, Lei Zhou, Xiao Bai, Yifei Huang, Lin Gu, Jun Zhou, Tatsuya Harada",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Goal-Oriented_Gaze_Estimation_for_Zero-Shot_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2044",
      "paper_id": "",
      "title": "GOAL: Global-local Object Alignment Learning",
      "authors": "Hyungyu Choi, Young Kyun Jang, Chanho Eom",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Choi_GOAL_Global-local_Object_Alignment_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_207",
      "paper_id": "",
      "title": "Gradient-based Parameter Selection for Efficient Fine-Tuning",
      "authors": "Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, Shanghang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Gradient-based_Parameter_Selection_for_Efficient_Fine-Tuning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_685",
      "paper_id": "",
      "title": "Grounded Language-Image Pre-Training",
      "authors": "Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_939",
      "paper_id": "",
      "title": "Grounding Everything: Emerging Localization Properties in Vision-Language Transformers",
      "authors": "Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bousselham_Grounding_Everything_Emerging_Localization_Properties_in_Vision-Language_Transformers_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1496",
      "paper_id": "",
      "title": "Hardness Sampling for Self-Training Based Transductive Zero-Shot Learning",
      "authors": "Liu Bo, Qiulei Dong, Zhanyi Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Bo_Hardness_Sampling_for_Self-Training_Based_Transductive_Zero-Shot_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2356",
      "paper_id": "",
      "title": "Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment",
      "authors": "Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Ankit Singh, Noel E. O'Connor",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Maniparambil_Harnessing_Frozen_Unimodal_Encoders_for_Flexible_Multimodal_Alignment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1588",
      "paper_id": "",
      "title": "Hierarchical Knowledge Prompt Tuning for Multi-task Test-Time Adaptation",
      "authors": "Qiang Zhang, Mengsheng Zhao, Jiawei Liu, Fanrui Zhang, Yongchao Xu, Zheng-Jun Zha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Hierarchical_Knowledge_Prompt_Tuning_for_Multi-task_Test-Time_Adaptation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1123",
      "paper_id": "",
      "title": "Hierarchical Prompt Learning for Multi-Task Learning",
      "authors": "Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu, Zhuokun Yao, Baofeng Zhang, Zhiwei Xiong, Chenguang Gui",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2225",
      "paper_id": "",
      "title": "Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning",
      "authors": "Zhongzhi Yu, Shang Wu, Yonggan Fu, Shunyao Zhang, Yingyan (Celine) Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2141",
      "paper_id": "",
      "title": "HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models",
      "authors": "Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, Hengshuang Zhao, Hang Xu, Lu Hou, Wei Zhang, Xiaodan Liang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_HiRes-LLaVA_Restoring_Fragmentation_Input_in_High-Resolution_Large_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1576",
      "paper_id": "",
      "title": "How to Configure Good In-Context Sequence for Visual Question Answering",
      "authors": "Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, Xu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_How_to_Configure_Good_In-Context_Sequence_for_Visual_Question_Answering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2294",
      "paper_id": "",
      "title": "How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?",
      "authors": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Koley_How_to_Handle_Sketch-Abstraction_in_Sketch-Based_Image_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2680",
      "paper_id": "",
      "title": "How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?",
      "authors": "Yuxin Chen, Zongyang Ma, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Ying Shan, Xiaojuan Qi, Weiming Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_How_to_Make_Cross_Encoder_a_Good_Teacher_for_Efficient_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2658",
      "paper_id": "",
      "title": "How to Merge Your Multimodal Models Over Time?",
      "authors": "Sebastian Dziadzio, Vishaal Udandarao, Karsten Roth, Ameya Prabhu, Zeynep Akata, Samuel Albanie, Matthias Bethge",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Dziadzio_How_to_Merge_Your_Multimodal_Models_Over_Time_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_449",
      "paper_id": "",
      "title": "Hyperbolic Learning with Synthetic Captions for Open-World Detection",
      "authors": "Fanjie Kong, Yanbei Chen, Jiarui Cai, Davide Modolo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kong_Hyperbolic_Learning_with_Synthetic_Captions_for_Open-World_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1876",
      "paper_id": "",
      "title": "Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception",
      "authors": "Luke Chen, Junyao Wang, Trier Mortlock, Pramod Khargonekar, Mohammad Abdullah Al Faruque",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Hyperdimensional_Uncertainty_Quantification_for_Multimodal_Uncertainty_Fusion_in_Autonomous_Vehicles_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1449",
      "paper_id": "",
      "title": "I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification",
      "authors": "Muhammad Ferjad Naeem, Muhammad Gul Zain Ali Khan, Yongqin Xian, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool, Federico Tombari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1938",
      "paper_id": "",
      "title": "iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition",
      "authors": "Yixuan Wei, Yue Cao, Zheng Zhang, Houwen Peng, Zhuliang Yao, Zhenda Xie, Han Hu, Baining Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1994",
      "paper_id": "",
      "title": "ILIAS: Instance-Level Image retrieval At Scale",
      "authors": "Giorgos Kordopatis-Zilos, Vladan Stojnić, Anna Manko, Pavel Suma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, Jiri Matas, Ondrej Chum, Giorgos Tolias",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kordopatis-Zilos_ILIAS_Instance-Level_Image_retrieval_At_Scale_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1407",
      "paper_id": "",
      "title": "Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks",
      "authors": "Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_367",
      "paper_id": "",
      "title": "Image Change Captioning by Learning From an Auxiliary Task",
      "authors": "Mehrdad Hosseinzadeh, Yang Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hosseinzadeh_Image_Change_Captioning_by_Learning_From_an_Auxiliary_Task_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_655",
      "paper_id": "",
      "title": "Image Segmentation Using Text and Image Prompts",
      "authors": "Timo Lüddecke, Alexander Ecker",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_801",
      "paper_id": "",
      "title": "ImageBind: One Embedding Space To Bind Them All",
      "authors": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1149",
      "paper_id": "",
      "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning",
      "authors": "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2471",
      "paper_id": "",
      "title": "Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy",
      "authors": "You Li, Fan Ma, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Imagine_and_Seek_Improving_Composed_Image_Retrieval_with_an_Imagined_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2456",
      "paper_id": "",
      "title": "ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning",
      "authors": "Haoyuan Yang, Xiaoou Li, Jiaming Lv, Xianjun Cheng, Qilong Wang, Peihua Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2359",
      "paper_id": "",
      "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
      "authors": "Oindrila Saha, Grant Van Horn, Subhransu Maji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Saha_Improved_Zero-Shot_Classification_by_Adapting_VLMs_with_Text_Descriptions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1873",
      "paper_id": "",
      "title": "Improving Cross-Modal Retrieval With Set of Diverse Embeddings",
      "authors": "Dongwon Kim, Namyup Kim, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Improving_Cross-Modal_Retrieval_With_Set_of_Diverse_Embeddings_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_993",
      "paper_id": "",
      "title": "Improving Generalized Zero-Shot Learning by Exploring the Diverse Semantics from External Class Names",
      "authors": "Yapeng Li, Yong Luo, Zengmao Wang, Bo Du",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Improving_Generalized_Zero-Shot_Learning_by_Exploring_the_Diverse_Semantics_from_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_357",
      "paper_id": "",
      "title": "Improving Image Recognition by Retrieving From Web-Scale Image-Text Data",
      "authors": "Ahmet Iscen, Alireza Fathi, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Iscen_Improving_Image_Recognition_by_Retrieving_From_Web-Scale_Image-Text_Data_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1574",
      "paper_id": "",
      "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates",
      "authors": "Fiona Ryan, Josef Sivic, Fabian Caba Heilbron, Judy Hoffman, James M. Rehg, Bryan Russell",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_844",
      "paper_id": "",
      "title": "Improving Zero-Shot Generalization and Robustness of Multi-Modal Models",
      "authors": "Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, Jiaping Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2071",
      "paper_id": "",
      "title": "Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models",
      "authors": "Yuhao Cui, Xinxing Zu, Wenhua Zhang, Zhongzhou Zhao, Jinyang Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_Incorporating_Dense_Knowledge_Alignment_into_Unified_Multimodal_Representation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_632",
      "paper_id": "",
      "title": "Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition",
      "authors": "Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Buettner_Incorporating_Geo-Diverse_Knowledge_into_Prompting_for_Increased_Geographical_Robustness_in_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_847",
      "paper_id": "",
      "title": "Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding",
      "authors": "Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1334",
      "paper_id": "",
      "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
      "authors": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_291",
      "paper_id": "",
      "title": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding",
      "authors": "Morris Alper, Michael Fiman, Hadar Averbuch-Elor",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Alper_Is_BERT_Blind_Exploring_the_Effect_of_Vision-and-Language_Pretraining_on_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_633",
      "paper_id": "",
      "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data",
      "authors": "Dominik Schnaus, Nikita Araslanov, Daniel Cremers",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Schnaus_Its_a_Blind_Match_Towards_Vision-Language_Correspondence_without_Parallel_Data_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2017",
      "paper_id": "",
      "title": "IterIS: Iterative Inference-Solving Alignment for LoRA Merging",
      "authors": "Hongxu Chen, Zhen Wang, Runshi Li, Bowei Zhu, Long Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_IterIS_Iterative_Inference-Solving_Alignment_for_LoRA_Merging_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_893",
      "paper_id": "",
      "title": "JoAPR: Cleaning the Lens of Prompt Learning for Vision-Language Models",
      "authors": "Yuncheng Guo, Xiaodong Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_JoAPR_Cleaning_the_Lens_of_Prompt_Learning_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_625",
      "paper_id": "",
      "title": "Joint Scheduling of Causal Prompts and Tasks for Multi-Task Learning",
      "authors": "Chaoyang Li, Jianyang Qin, Jinhao Cui, Zeyu Liu, Ning Hu, Qing Liao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Joint_Scheduling_of_Causal_Prompts_and_Tasks_for_Multi-Task_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1437",
      "paper_id": "",
      "title": "Kaleido-BERT: Vision-Language Pre-Training on Fashion Domain",
      "authors": "Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhuge_Kaleido-BERT_Vision-Language_Pre-Training_on_Fashion_Domain_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_178",
      "paper_id": "",
      "title": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation",
      "authors": "Jiaxin Cai, Jingze Su, Qi Li, Wenjie Yang, Shu Wang, Tiesong Zhao, Shengfeng He, Wenxi Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_2050",
      "paper_id": "",
      "title": "KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning",
      "authors": "Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2775",
      "paper_id": "",
      "title": "Knowledge Bridger: Towards Training-Free Missing Modality Completion",
      "authors": "Guanzhou Ke, Shengfeng He, Xiaoli Wang, Bo Wang, Guoqing Chao, Yuanyang Zhang, Yi Xie, Hexing Su",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ke_Knowledge_Bridger_Towards_Training-Free_Missing_Modality_Completion_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2242",
      "paper_id": "",
      "title": "Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval",
      "authors": "Yucheng Suo, Fan Ma, Linchao Zhu, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Suo_Knowledge-Enhanced_Dual-stream_Zero-shot_Composed_Image_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_178",
      "paper_id": "",
      "title": "Label Propagation for Zero-shot Classification with Vision-Language Models",
      "authors": "Vladan Stojni?, Yannis Kalantidis, Giorgos Tolias",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Stojni_Label_Propagation_for_Zero-shot_Classification_with_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1771",
      "paper_id": "",
      "title": "LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant",
      "authors": "Yikun Liu, Yajie Zhang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1285",
      "paper_id": "",
      "title": "Language Models as Black-Box Optimizers for Vision-Language Models",
      "authors": "Shihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Language_Models_as_Black-Box_Optimizers_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1343",
      "paper_id": "",
      "title": "Language-conditioned Detection Transformer",
      "authors": "Jang Hyun Cho, Philipp Krähenbühl",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cho_Language-conditioned_Detection_Transformer_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1470",
      "paper_id": "",
      "title": "Language-only Training of Zero-shot Composed Image Retrieval",
      "authors": "Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, Sangdoo Yun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_Language-only_Training_of_Zero-shot_Composed_Image_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2608",
      "paper_id": "",
      "title": "Large Language Models are Good Prompt Learners for Low-Shot Image Classification",
      "authors": "Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, Ram Nevatia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Large_Language_Models_are_Good_Prompt_Learners_for_Low-Shot_Image_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2015",
      "paper_id": "",
      "title": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models",
      "authors": "Adrian Bulat, Georgios Tzimiropoulos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_628",
      "paper_id": "",
      "title": "Learning Attention As Disentangler for Compositional Zero-Shot Learning",
      "authors": "Shaozhe Hao, Kai Han, Kwan-Yee K. Wong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Learning_Attention_As_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2081",
      "paper_id": "",
      "title": "Learning Attribute and Class-Specific Representation Duet for Fine-Grained Fashion Analysis",
      "authors": "Yang Jiao, Yan Gao, Jingjing Meng, Jin Shang, Yi Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1946",
      "paper_id": "",
      "title": "Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection",
      "authors": "Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang Lin, Guanbin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_Background_Prompts_to_Discover_Implicit_Knowledge_for_Open_Vocabulary_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_267",
      "paper_id": "",
      "title": "Learning Conditional Attributes for Compositional Zero-Shot Learning",
      "authors": "Qingsheng Wang, Lingqiao Liu, Chenchen Jing, Hao Chen, Guoqiang Liang, Peng Wang, Chunhua Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Conditional_Attributes_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_818",
      "paper_id": "",
      "title": "Learning Cross-Modal Retrieval With Noisy Labels",
      "authors": "Peng Hu, Xi Peng, Hongyuan Zhu, Liangli Zhen, Jie Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_328",
      "paper_id": "",
      "title": "Learning Customized Visual Models With Retrieval-Augmented Knowledge",
      "authors": "Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, Chunyuan Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1520",
      "paper_id": "",
      "title": "Learning Expressive Prompting With Residuals for Vision Transformers",
      "authors": "Rajshekhar Das, Yonatan Dukler, Avinash Ravichandran, Ashwin Swaminathan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Learning_Expressive_Prompting_With_Residuals_for_Vision_Transformers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_755",
      "paper_id": "",
      "title": "Learning Geometry-Aware Representations by Sketching",
      "authors": "Hyundo Lee, Inwoo Hwang, Hyunsung Go, Won-Seok Choi, Kibeom Kim, Byoung-Tak Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Learning_Geometry-Aware_Representations_by_Sketching_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_554",
      "paper_id": "",
      "title": "Learning Graph Embeddings for Compositional Zero-Shot Learning",
      "authors": "Muhammad Ferjad Naeem, Yongqin Xian, Federico Tombari, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Naeem_Learning_Graph_Embeddings_for_Compositional_Zero-Shot_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_535",
      "paper_id": "",
      "title": "Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce",
      "authors": "Yang Jin, Yongzhi Li, Zehuan Yuan, Yadong Mu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Learning_Instance-Level_Representation_for_Large-Scale_Multi-Modal_Pretraining_in_E-Commerce_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2597",
      "paper_id": "",
      "title": "Learning on Model Weights using Tree Experts",
      "authors": "Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Horwitz_Learning_on_Model_Weights_using_Tree_Experts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_642",
      "paper_id": "",
      "title": "Learning Semantic Relationship Among Instances for Image-Text Matching",
      "authors": "Zheren Fu, Zhendong Mao, Yan Song, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_423",
      "paper_id": "",
      "title": "Learning To Detect and Segment for Open Vocabulary Object Detection",
      "authors": "Tao Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_To_Detect_and_Segment_for_Open_Vocabulary_Object_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_418",
      "paper_id": "",
      "title": "Learning To Name Classes for Vision and Language Models",
      "authors": "Sarah Parisot, Yongxin Yang, Steven McDonagh",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_824",
      "paper_id": "",
      "title": "Learning To Predict Visual Attributes in the Wild",
      "authors": "Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Pham_Learning_To_Predict_Visual_Attributes_in_the_Wild_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_622",
      "paper_id": "",
      "title": "Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model",
      "authors": "Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, Guoqi Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Learning_To_Prompt_for_Open-Vocabulary_Object_Detection_With_Vision-Language_Model_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1591",
      "paper_id": "",
      "title": "Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval",
      "authors": "Haochen Han, Qinghua Zheng, Guang Dai, Minnan Luo, Jingdong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Han_Learning_to_Rematch_Mismatched_Pairs_for_Robust_Cross-Modal_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2412",
      "paper_id": "",
      "title": "Learning Vision from Models Rivals Learning Vision from Data",
      "authors": "Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Learning_Vision_from_Models_Rivals_Learning_Vision_from_Data_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1642",
      "paper_id": "",
      "title": "Learning Visual Composition through Improved Semantic Guidance",
      "authors": "Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao, Kaifeng Chen, Abhijit Ogale, Jonathon Shlens",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Stone_Learning_Visual_Composition_through_Improved_Semantic_Guidance_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1964",
      "paper_id": "",
      "title": "Learning Visual Representations via Language-Guided Sampling",
      "authors": "Mohamed El Banani, Karan Desai, Justin Johnson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1782",
      "paper_id": "",
      "title": "Learning with Noisy Triplet Correspondence for Composed Image Retrieval",
      "authors": "Shuxian Li, Changhao He, Xiting Liu, Joey Tianyi Zhou, Xi Peng, Peng Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Learning_with_Noisy_Triplet_Correspondence_for_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2608",
      "paper_id": "",
      "title": "Less is More: Efficient Model Merging with Binary Task Switch",
      "authors": "Biqing Qi, Fangyuan Li, Zhen Wang, Junqi Gao, Dong Li, Peng Ye, Bowen Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1326",
      "paper_id": "",
      "title": "Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition",
      "authors": "Zheda Mai, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Quang-Huy Nguyen, Li Zhang, Wei-Lun Chao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_630",
      "paper_id": "",
      "title": "Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification",
      "authors": "Chao Yi, Lu Ren, De-Chuan Zhan, Han-Jia Ye",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_Leveraging_Cross-Modal_Neighbor_Representation_for_Improved_CLIP_Classification_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1475",
      "paper_id": "",
      "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-Training",
      "authors": "Yunhao Gou, Tom Ko, Hansi Yang, James Kwok, Yu Zhang, Mingxuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1826",
      "paper_id": "",
      "title": "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification",
      "authors": "Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R. Venkatesh Babu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Addepalli_Leveraging_Vision-Language_Models_for_Improving_Domain_Generalization_in_Image_Classification_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_712",
      "paper_id": "",
      "title": "Libra-Merging: Importance-redundancy and Pruning-merging Trade-off for Acceleration Plug-in in Large Vision-Language Model",
      "authors": "Longrong Yang, Dong Shen, Chaoxiang Cai, Kaibing Chen, Fan Yang, Tingting Gao, Di Zhang, Xi Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Libra-Merging_Importance-redundancy_and_Pruning-merging_Trade-off_for_Acceleration_Plug-in_in_Large_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_629",
      "paper_id": "",
      "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference",
      "authors": "Hao Yin, Guangzong Si, Zilei Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_Lifting_the_Veil_on_Visual_Information_Flow_in_MLLMs_Unlocking_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1006",
      "paper_id": "",
      "title": "Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment",
      "authors": "Zheren Fu, Lei Zhang, Hou Xia, Zhendong Mao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Fu_Linguistic-Aware_Patch_Slimming_Framework_for_Fine-grained_Cross-Modal_Alignment_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1477",
      "paper_id": "",
      "title": "Link-Context Learning for Multimodal LLMs",
      "authors": "Yan Tai, Weichen Fan, Zhao Zhang, Ziwei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_66",
      "paper_id": "",
      "title": "LiT: Zero-Shot Transfer With Locked-Image Text Tuning",
      "authors": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2268",
      "paper_id": "",
      "title": "LLaFS: When Large Language Models Meet Few-Shot Segmentation",
      "authors": "Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_LLaFS_When_Large_Language_Models_Meet_Few-Shot_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_164",
      "paper_id": "",
      "title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction",
      "authors": "Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zou_LLaMA-Excitor_General_Instruction_Tuning_via_Indirect_Feature_Interaction_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2078",
      "paper_id": "",
      "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
      "authors": "Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2028",
      "paper_id": "",
      "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling",
      "authors": "Zilin Xiao, Pavel Suma, Ayush Sachdeva, Hao-Jen Wang, Giorgos Kordopatis-Zilos, Giorgos Tolias, Vicente Ordonez",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_LOCORE_Image_Re-ranking_with_Long-Context_Sequence_Modeling_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_445",
      "paper_id": "",
      "title": "LOGICZSL: Exploring Logic-induced Representation for Compositional Zero-shot Learning",
      "authors": "Peng Wu, Xiankai Lu, Hao Hu, Yongqin Xian, Jianbing Shen, Wenguan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_LOGICZSL_Exploring_Logic-induced_Representation_for_Compositional_Zero-shot_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_918",
      "paper_id": "",
      "title": "Logits DeConfusion with CLIP for Few-Shot Learning",
      "authors": "Shuo Li, Fang Liu, Zehua Hao, Xinyi Wang, Lingling Li, Xu Liu, Puhua Chen, Wenping Ma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Logits_DeConfusion_with_CLIP_for_Few-Shot_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_572",
      "paper_id": "",
      "title": "LoKi: Low-dimensional KAN for Efficient Fine-tuning Image Models",
      "authors": "Xuan Cai, Renjie Pan, Hua Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_LoKi_Low-dimensional_KAN_for_Efficient_Fine-tuning_Image_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1913",
      "paper_id": "",
      "title": "LoRA Recycle: Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs",
      "authors": "Zixuan Hu, Yongxian Wei, Li Shen, Chun Yuan, Dacheng Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_LoRA_Recycle_Unlocking_Tuning-Free_Few-Shot_Adaptability_in_Visual_Foundation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1276",
      "paper_id": "",
      "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
      "authors": "Jian Liang, Wenke Huang, Guancheng Wan, Qu Yang, Mang Ye",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_392",
      "paper_id": "",
      "title": "LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking",
      "authors": "Jialin Li, Qiang Nie, Weifu Fu, Yuhuan Lin, Guangpin Tao, Yong Liu, Chengjie Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LORS_Low-rank_Residual_Structure_for_Parameter-Efficient_Network_Stacking_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1681",
      "paper_id": "",
      "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table",
      "authors": "Yusuke Matsui",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Matsui_LotusFilter_Fast_Diverse_Nearest_Neighbor_Search_via_a_Learned_Cutoff_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1612",
      "paper_id": "",
      "title": "Low-Biased General Annotated Dataset Generation",
      "authors": "Dengyang Jiang, Haoyu Wang, Lei Zhang, Wei Wei, Guang Dai, Mengmeng Wang, Jingdong Wang, Yanning Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Low-Biased_General_Annotated_Dataset_Generation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1143",
      "paper_id": "",
      "title": "Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs",
      "authors": "Lin Song, Yukang Chen, Shuai Yang, Xiaohan Ding, Yixiao Ge, Ying-Cong Chen, Ying Shan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Low-Rank_Approximation_for_Sparse_Attention_in_Multi-Modal_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_842",
      "paper_id": "",
      "title": "Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach",
      "authors": "Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, Yang Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Dong_Low-Rank_Rescaled_Vision_Transformer_Fine-Tuning_A_Residual_Design_Approach_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1396",
      "paper_id": "",
      "title": "Low-Resource Vision Challenges for Foundation Models",
      "authors": "Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Low-Resource_Vision_Challenges_for_Foundation_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2539",
      "paper_id": "",
      "title": "LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP",
      "authors": "Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, Ismail Ben Ayed",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_LP_A_Surprisingly_Strong_Linear_Probe_for_Few-Shot_CLIP_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_763",
      "paper_id": "",
      "title": "LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images",
      "authors": "Jing Zhang, Irving Fang, Hao Wu, Akshat Kaushik, Alice Rodriguez, Hanwen Zhao, Juexiao Zhang, Zhuo Zheng, Radu Iovita, Chen Feng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_LUWA_Dataset_Learning_Lithic_Use-Wear_Analysis_on_Microscopic_Images_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1126",
      "paper_id": "",
      "title": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-Training",
      "authors": "Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang, Nan Duan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1693",
      "paper_id": "",
      "title": "M5Product: Self-Harmonized Contrastive Learning for E-Commercial Multi-Modal Pretraining",
      "authors": "Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C. Kampffmeyer, Xiaoyong Wei, Minlong Lu, Yaowei Wang, Xiaodan Liang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_M5Product_Self-Harmonized_Contrastive_Learning_for_E-Commercial_Multi-Modal_Pretraining_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2519",
      "paper_id": "",
      "title": "MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer",
      "authors": "Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_954",
      "paper_id": "",
      "title": "MAFA: Managing False Negatives for Vision-Language Pre-training",
      "authors": "Jaeseok Byun, Dohoon Kim, Taesup Moon",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Byun_MAFA_Managing_False_Negatives_for_Vision-Language_Pre-training_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2131",
      "paper_id": "",
      "title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model",
      "authors": "Yatai Ji, Junjie Wang, Yuan Gong, Lin Zhang, Yanru Zhu, Hongfa Wang, Jiaxing Zhang, Tetsuya Sakai, Yujiu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_MAP_Multimodal_Uncertainty-Aware_Vision-Language_Pre-Training_Model_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2026",
      "paper_id": "",
      "title": "MaPLe: Multi-Modal Prompt Learning",
      "authors": "Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1548",
      "paper_id": "",
      "title": "MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining",
      "authors": "Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1962",
      "paper_id": "",
      "title": "Masked AutoDecoder is Effective Multi-Task Vision Generalist",
      "authors": "Han Qiu, Jiaxing Huang, Peng Gao, Lewei Lu, Xiaoqin Zhang, Shijian Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Qiu_Masked_AutoDecoder_is_Effective_Multi-Task_Vision_Generalist_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_708",
      "paper_id": "",
      "title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale",
      "authors": "Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Weers_Masked_Autoencoding_Does_Not_Help_Natural_Language_Supervision_at_Scale_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_952",
      "paper_id": "",
      "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
      "authors": "Shiyao Li, Yingchun Hu, Xuefei Ning, Xihui Liu, Ke Hong, Xiaotao Jia, Xiuhong Li, Yaqi Yan, Pei Ran, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MBQ_Modality-Balanced_Quantization_for_Large_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_89",
      "paper_id": "",
      "title": "MeaCap: Memory-Augmented Zero-shot Image Captioning",
      "authors": "Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Bo Chen, Zhengjue Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_MeaCap_Memory-Augmented_Zero-shot_Image_Captioning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_563",
      "paper_id": "",
      "title": "Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning",
      "authors": "Zichen Tian, Yaoyao Liu, Qianru Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_923",
      "paper_id": "",
      "title": "MetaCLUE: Towards Comprehensive Visual Metaphors Research",
      "authors": "Arjun R. Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo, Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas, William T. Freeman, Yuanzhen Li, Varun Jampani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_397",
      "paper_id": "",
      "title": "MICAS: Multi-grained In-Context Adaptive Sampling for 3D Point Cloud Processing",
      "authors": "Feifei Shao, Ping Liu, Zhao Wang, Yawei Luo, Hongwei Wang, Jun Xiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shao_MICAS_Multi-grained_In-Context_Adaptive_Sampling_for_3D_Point_Cloud_Processing_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2705",
      "paper_id": "",
      "title": "Mimic In-Context Learning for Multimodal Tasks",
      "authors": "Yuchu Jiang, Jiale Fu, Chenduo Hao, Xinting Hu, Yingzhe Peng, Xin Geng, Xu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Mimic_In-Context_Learning_for_Multimodal_Tasks_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1200",
      "paper_id": "",
      "title": "Minimal Interaction Seperated Tuning: A New Paradigm for Visual Adaptation",
      "authors": "Ningyuan Tang, Minghao Fu, Jianxin Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Minimal_Interaction_Seperated_Tuning_A_New_Paradigm_for_Visual_Adaptation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_22",
      "paper_id": "",
      "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval",
      "authors": "Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Gaopeng Gou, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Missing_Target-Relevant_Information_Prediction_with_World_Model_for_Accurate_Zero-Shot_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_411",
      "paper_id": "",
      "title": "Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning",
      "authors": "Zihua Zhao, Mengxi Chen, Tianjie Dai, Jiangchao Yao, Bo Han, Ya Zhang, Yanfeng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Mitigating_Noisy_Correspondence_by_Geometrical_Structure_Consistency_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2104",
      "paper_id": "",
      "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
      "authors": "Jingjing Jiang, Nanning Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MixPHM_Redundancy-Aware_Parameter-Efficient_Tuning_for_Low-Resource_Visual_Question_Answering_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2649",
      "paper_id": "",
      "title": "MMA: Multi-Modal Adapter for Vision-Language Models",
      "authors": "Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_352",
      "paper_id": "",
      "title": "MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning",
      "authors": "Shicai Wei, Chunbo Luo, Yang Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1494",
      "paper_id": "",
      "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
      "authors": "Yuncheng Guo, Xiaodong Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_MMRL_Multi-Modal_Representation_Learning_for_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_465",
      "paper_id": "",
      "title": "Mobile User Interface Element Detection via Adaptively Prompt Tuning",
      "authors": "Zhangxuan Gu, Zhuoer Xu, Haoxing Chen, Jun Lan, Changhua Meng, Weiqiang Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1601",
      "paper_id": "",
      "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
      "authors": "Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_802",
      "paper_id": "",
      "title": "MoDE: CLIP Data Experts via Clustering",
      "authors": "Jiawei Ma, Po-Yao Huang, Saining Xie, Shang-Wen Li, Luke Zettlemoyer, Shih-Fu Chang, Wen-Tau Yih, Hu Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_MoDE_CLIP_Data_Experts_via_Clustering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_303",
      "paper_id": "",
      "title": "Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use",
      "authors": "Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, Ranjay Krishna, Ariel Fuxman, Tom Duerig",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Toubal_Modeling_Collaborator_Enabling_Subjective_Vision_Classification_With_Minimal_Human_Effort_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1061",
      "paper_id": "",
      "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training",
      "authors": "Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jiawen Liu, Jifeng Dai, Yu Qiao, Xizhou Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_Mono-InternVL_Pushing_the_Boundaries_of_Monolithic_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2411",
      "paper_id": "",
      "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric",
      "authors": "Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_MoPE-CLIP_Structured_Pruning_for_Efficient_Vision-Language_Models_with_Module-wise_Pruning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_409",
      "paper_id": "",
      "title": "More Photos Are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval",
      "authors": "Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Yongxin Yang, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Bhunia_More_Photos_Are_All_You_Need_Semi-Supervised_Learning_for_Fine-Grained_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2809",
      "paper_id": "",
      "title": "MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning",
      "authors": "Xu Han, Yuan Tang, Jinfeng Xu, Xianzhi Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_MoST_Efficient_Monarch_Sparse_Tuning_for_3D_Representation_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1331",
      "paper_id": "",
      "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
      "authors": "Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An, Ningning Ma, Shanghang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cao_MoVE-KD_Knowledge_Distillation_for_VLMs_with_Mixture_of_Visual_Encoders_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1473",
      "paper_id": "",
      "title": "MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning",
      "authors": "Shiming Chen, Ziming Hong, Guo-Sen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, Jian Zhao, Xinge You",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_MSDN_Mutually_Semantic_Distillation_Network_for_Zero-Shot_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2146",
      "paper_id": "",
      "title": "MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning",
      "authors": "Ahmed Agiza, Marina Neseem, Sherief Reda",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Agiza_MTLoRA_Low-Rank_Adaptation_Approach_for_Efficient_Multi-Task_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1252",
      "paper_id": "",
      "title": "MulT: An End-to-End Multitask Learning Transformer",
      "authors": "Deblina Bhattacharjee, Tong Zhang, Sabine Süsstrunk, Mathieu Salzmann",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bhattacharjee_MulT_An_End-to-End_Multitask_Learning_Transformer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1167",
      "paper_id": "",
      "title": "Multi-Modal Alignment Using Representation Codebook",
      "authors": "Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Multi-Modal_Alignment_Using_Representation_Codebook_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1531",
      "paper_id": "",
      "title": "Multi-Modal Extreme Classification",
      "authors": "Anshul Mittal, Kunal Dahiya, Shreya Malani, Janani Ramaswamy, Seba Kuruvilla, Jitendra Ajmera, Keng-hao Chang, Sumeet Agarwal, Purushottam Kar, Manik Varma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Mittal_Multi-Modal_Extreme_Classification_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1933",
      "paper_id": "",
      "title": "Multi-Modal Proxy Learning Towards Personalized Visual Multiple Clustering",
      "authors": "Jiawei Yao, Qi Qian, Juhua Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_Multi-Modal_Proxy_Learning_Towards_Personalized_Visual_Multiple_Clustering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2032",
      "paper_id": "",
      "title": "Multi-Modal Representation Learning With Text-Driven Soft Masks",
      "authors": "Jaeyoo Park, Bohyung Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Multi-Modal_Representation_Learning_With_Text-Driven_Soft_Masks_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2266",
      "paper_id": "",
      "title": "MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning",
      "authors": "Matteo Farina, Massimiliano Mancini, Elia Cunegatti, Gaowen Liu, Giovanni Iacca, Elisa Ricci",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Farina_MULTIFLOW_Shifting_Towards_Task-Agnostic_Vision-Language_Pruning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1083",
      "paper_id": "",
      "title": "Multilateral Semantic Relations Modeling for Image Text Retrieval",
      "authors": "Zheng Wang, Zhenwei Gao, Kangshuai Guo, Yang Yang, Xiaoming Wang, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_293",
      "paper_id": "",
      "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
      "authors": "Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor G. Turrisi da Costa, Louis Béthune, Zhe Gan, Alexander Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua Susskind, Alaaeldin El-Nouby",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1339",
      "paper_id": "",
      "title": "Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification",
      "authors": "Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, Jianhua Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2070",
      "paper_id": "",
      "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
      "authors": "Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Multimodal_Pathway_Improve_Transformers_with_Irrelevant_Data_from_Other_Modalities_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_564",
      "paper_id": "",
      "title": "Multimodal Prompting With Missing Modalities for Visual Recognition",
      "authors": "Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, Chen-Yu Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2618",
      "paper_id": "",
      "title": "Multimodal Representation Learning by Alternating Unimodal Adaptation",
      "authors": "Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Multimodal_Representation_Learning_by_Alternating_Unimodal_Adaptation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_672",
      "paper_id": "",
      "title": "Multimodal Token Fusion for Vision Transformers",
      "authors": "Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, Yunhe Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_485",
      "paper_id": "",
      "title": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models",
      "authors": "Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1416",
      "paper_id": "",
      "title": "Mutual Quantization for Cross-Modal Search With Noisy Labels",
      "authors": "Erkun Yang, Dongren Yao, Tongliang Liu, Cheng Deng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Mutual_Quantization_for_Cross-Modal_Search_With_Noisy_Labels_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1597",
      "paper_id": "",
      "title": "Negative-Aware Attention Framework for Image-Text Matching",
      "authors": "Kun Zhang, Zhendong Mao, Quan Wang, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Negative-Aware_Attention_Framework_for_Image-Text_Matching_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1619",
      "paper_id": "",
      "title": "NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval",
      "authors": "Zengrong Lin, Zheng Wang, Tianwen Qian, Pan Mu, Sixian Chan, Cong Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_NeighborRetr_Balancing_Hub_Centrality_in_Cross-Modal_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1501",
      "paper_id": "",
      "title": "Neural Congealing: Aligning Images to a Joint Semantic Atlas",
      "authors": "Dolev Ofri-Amar, Michal Geyer, Yoni Kasten, Tali Dekel",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ofri-Amar_Neural_Congealing_Aligning_Images_to_a_Joint_Semantic_Atlas_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_205",
      "paper_id": "",
      "title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
      "authors": "Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1388",
      "paper_id": "",
      "title": "NOC-REK: Novel Object Captioning With Retrieved Vocabulary From External Knowledge",
      "authors": "Duc Minh Vo, Hong Chen, Akihiro Sugimoto, Hideki Nakayama",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Vo_NOC-REK_Novel_Object_Captioning_With_Retrieved_Vocabulary_From_External_Knowledge_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2173",
      "paper_id": "",
      "title": "Noisy Correspondence Learning With Meta Similarity Correction",
      "authors": "Haochen Han, Kaiyao Miao, Qinghua Zheng, Minnan Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1520",
      "paper_id": "",
      "title": "Non-autoregressive Sequence-to-Sequence Vision-Language Models",
      "authors": "Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_Non-autoregressive_Sequence-to-Sequence_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2321",
      "paper_id": "",
      "title": "Non-Contrastive Learning Meets Language-Image Pre-Training",
      "authors": "Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, Furu Wei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_494",
      "paper_id": "",
      "title": "Non-Generative Generalized Zero-Shot Learning via Task-Correlated Disentanglement and Controllable Samples Synthesis",
      "authors": "Yaogong Feng, Xiaowen Huang, Pengbo Yang, Jian Yu, Jitao Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Feng_Non-Generative_Generalized_Zero-Shot_Learning_via_Task-Correlated_Disentanglement_and_Controllable_Samples_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2054",
      "paper_id": "",
      "title": "NVILA: Efficient Frontier Visual Language Models",
      "authors": "Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Haotian Tang, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Jinyi Hu, Sifei Liu, Ranjay Krishna, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_NVILA_Efficient_Frontier_Visual_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2640",
      "paper_id": "",
      "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models",
      "authors": "Ashshak Sharifdeen, Muhammad Akhtar Munir, Sanoojan Baliah, Salman Khan, Muhammad Haris Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_621",
      "paper_id": "",
      "title": "Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection",
      "authors": "Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, Si Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_602",
      "paper_id": "",
      "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
      "authors": "Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, Radu Soricut",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Omni-SMoLA_Boosting_Generalist_Multimodal_Models_with_Soft_Mixture_of_Low-rank_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_680",
      "paper_id": "",
      "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos",
      "authors": "Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2469",
      "paper_id": "",
      "title": "OmniVec2 - A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning",
      "authors": "Siddharth Srivastava, Gaurav Sharma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Srivastava_OmniVec2_-_A_Novel_Transformer_based_Network_for_Large_Scale_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_946",
      "paper_id": "",
      "title": "Omnivore: A Single Model for Many Visual Modalities",
      "authors": "Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Girdhar_Omnivore_A_Single_Model_for_Many_Visual_Modalities_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_723",
      "paper_id": "",
      "title": "On Scaling Up a Multilingual Vision and Language Model",
      "authors": "Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_On_Scaling_Up_a_Multilingual_Vision_and_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2680",
      "paper_id": "",
      "title": "On the Out-Of-Distribution Generalization of Large Multimodal Models",
      "authors": "Xingxuan Zhang, Jiansheng Li, Wenjing Chu, junjia hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Liping Jing, Peng Cui",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_On_the_Out-Of-Distribution_Generalization_of_Large_Multimodal_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_790",
      "paper_id": "",
      "title": "On the Test-Time Zero-Shot Generalization of Vision-Language Models: Do We Really Need Prompt Learning?",
      "authors": "Maxime Zanella, Ismail Ben Ayed",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zanella_On_the_Test-Time_Zero-Shot_Generalization_of_Vision-Language_Models_Do_We_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_511",
      "paper_id": "",
      "title": "On Train-Test Class Overlap and Detection for Image Retrieval",
      "authors": "Chull Hwan Song, Jooyoung Yoon, Taebaek Hwang, Shunghyun Choi, Yeong Hyeon Gu, Yannis Avrithis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Song_On_Train-Test_Class_Overlap_and_Detection_for_Image_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1825",
      "paper_id": "",
      "title": "Once-Tuning-Multiple-Variants: Tuning Once and Expanded as Multiple Vision-Language Model Variants",
      "authors": "Chong Yu, Tao Chen, Zhongxue Gan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Once-Tuning-Multiple-Variants_Tuning_Once_and_Expanded_as_Multiple_Vision-Language_Model_Variants_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_5",
      "paper_id": "",
      "title": "One Loss for Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching",
      "authors": "Khoa D. Doan, Peng Yang, Ping Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Doan_One_Loss_for_Quantization_Deep_Hashing_With_Discrete_Wasserstein_Distributional_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_681",
      "paper_id": "",
      "title": "Open Ad-hoc Categorization with Contextualized Feature Learning",
      "authors": "Zilin Wang, Sangwoo Mo, Stella X. Yu, Sima Behpour, Liu Ren",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Open_Ad-hoc_Categorization_with_Contextualized_Feature_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_699",
      "paper_id": "",
      "title": "Open Vocabulary Semantic Scene Sketch Understanding",
      "authors": "Ahmed Bourouis, Judith E. Fan, Yulia Gryaditskaya",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bourouis_Open_Vocabulary_Semantic_Scene_Sketch_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_337",
      "paper_id": "",
      "title": "Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning",
      "authors": "Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip H.S. Torr, Ser-Nam Lim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_295",
      "paper_id": "",
      "title": "Open World Compositional Zero-Shot Learning",
      "authors": "Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Mancini_Open_World_Compositional_Zero-Shot_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_503",
      "paper_id": "",
      "title": "Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator",
      "authors": "Shijie Wang, Jianlong Chang, Haojie Li, Zhihui Wang, Wanli Ouyang, Qi Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Open-Set_Fine-Grained_Retrieval_via_Prompting_Vision-Language_Evaluator_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1296",
      "paper_id": "",
      "title": "Open-Vocabulary Attribute Detection",
      "authors": "María A. Bravo, Sudhanshu Mittal, Simon Ging, Thomas Brox",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_105",
      "paper_id": "",
      "title": "Open-Vocabulary Object Detection Using Captions",
      "authors": "Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zareian_Open-Vocabulary_Object_Detection_Using_Captions_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_603",
      "paper_id": "",
      "title": "Open-Vocabulary One-Stage Detection With Hierarchical Visual-Language Knowledge Distillation",
      "authors": "Zongyang Ma, Guan Luo, Jin Gao, Liang Li, Yuxin Chen, Shaoru Wang, Congxuan Zhang, Weiming Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Open-Vocabulary_One-Stage_Detection_With_Hierarchical_Visual-Language_Knowledge_Distillation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1425",
      "paper_id": "",
      "title": "OvarNet: Towards Open-Vocabulary Object Attribute Recognition",
      "authors": "Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_OvarNet_Towards_Open-Vocabulary_Object_Attribute_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2253",
      "paper_id": "",
      "title": "OVMR: Open-Vocabulary Recognition with Multi-Modal References",
      "authors": "Zehong Ma, Shiliang Zhang, Longhui Wei, Qi Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_OVMR_Open-Vocabulary_Recognition_with_Multi-Modal_References_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_225",
      "paper_id": "",
      "title": "PACO: Parts and Attributes of Common Objects",
      "authors": "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, Dhruv Mahajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1420",
      "paper_id": "",
      "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models",
      "authors": "Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Dhouib_PACT_Pruning_and_Clustering-Based_Token_Reduction_for_Faster_Visual_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_175",
      "paper_id": "",
      "title": "Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model",
      "authors": "Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, Qi Tian, Wei Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_Parameter_Efficient_Fine-tuning_via_Cross_Block_Orchestration_for_Segment_Anything_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2194",
      "paper_id": "",
      "title": "Parameter Efficient Mamba Tuning via Projector-targeted Diagonal-centric Linear Transformation",
      "authors": "Seokil Ham, Hee-Seon Kim, Sangmin Woo, Changick Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ham_Parameter_Efficient_Mamba_Tuning_via_Projector-targeted_Diagonal-centric_Linear_Transformation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_988",
      "paper_id": "",
      "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models",
      "authors": "Jenny Schmalfuss, Nadine Chang, Vibashan VS, Maying Shen, Andres Bruhn, Jose M. Alvarez",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Schmalfuss_PARC_A_Quantitative_Framework_Uncovering_the_Symmetries_within_Vision_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_173",
      "paper_id": "",
      "title": "Partially Does It: Towards Scene-Level FG-SBIR With Partial Input",
      "authors": "Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Aneeshan Sain, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chowdhury_Partially_Does_It_Towards_Scene-Level_FG-SBIR_With_Partial_Input_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_742",
      "paper_id": "",
      "title": "PELA: Learning Parameter-Efficient Models with Low-Rank Approximation",
      "authors": "Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_PELA_Learning_Parameter-Efficient_Models_with_Low-Rank_Approximation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_736",
      "paper_id": "",
      "title": "Personalized Outfit Recommendation With Learnable Anchors",
      "authors": "Zhi Lu, Yang Hu, Yan Chen, Bing Zeng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Personalized_Outfit_Recommendation_With_Learnable_Anchors_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_347",
      "paper_id": "",
      "title": "Photo Pre-Training, but for Sketch",
      "authors": "Ke Li, Kaiyue Pang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_351",
      "paper_id": "",
      "title": "Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval",
      "authors": "Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, Tomas Pfister",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1899",
      "paper_id": "",
      "title": "PLeaS - Merging Models with Permutations and Least Squares",
      "authors": "Anshul Nasery, Jonathan Hayase, Pang Wei Koh, Sewoong Oh",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nasery_PLeaS_-_Merging_Models_with_Permutations_and_Least_Squares_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1613",
      "paper_id": "",
      "title": "PMR: Prototypical Modal Rebalance for Multimodal Learning",
      "authors": "Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junxiao Wang, Song Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2341",
      "paper_id": "",
      "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning",
      "authors": "Song Wang, Xiaolu Liu, Lingdong Kong, Jianyun Xu, Chunyong Hu, Gongfan Fang, Wentong Li, Jianke Zhu, Xinchao Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_PointLoRA_Low-Rank_Adaptation_with_Token_Selection_for_Point_Cloud_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2046",
      "paper_id": "",
      "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
      "authors": "Shin'ya Yamaguchi, Dewei Feng, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yamaguchi_Post-pre-training_for_Modality_Alignment_in_Vision-Language_Foundation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2621",
      "paper_id": "",
      "title": "Pre-training Vision Models with Mandelbulb Variations",
      "authors": "Benjamin Naoto Chiche, Yuto Horikawa, Ryo Fujita",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chiche_Pre-training_Vision_Models_with_Mandelbulb_Variations_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_83",
      "paper_id": "",
      "title": "Prefix Conditioning Unifies Language and Label Supervision",
      "authors": "Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, Tomas Pfister",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_495",
      "paper_id": "",
      "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual Classification",
      "authors": "Xiangyan Qu, Gaopeng Gou, Jiamin Zhuang, Jing Yu, Kun Song, Qihao Wang, Yili Li, Gang Xiong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qu_ProAPO_Progressively_Automatic_Prompt_Optimization_for_Visual_Classification_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_282",
      "paper_id": "",
      "title": "Probabilistic Embeddings for Cross-Modal Retrieval",
      "authors": "Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio de Rezende, Yannis Kalantidis, Diane Larlus",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chun_Probabilistic_Embeddings_for_Cross-Modal_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2199",
      "paper_id": "",
      "title": "Probabilistic Prompt Learning for Dense Prediction",
      "authors": "Hyeongjun Kwon, Taeyong Song, Somi Jeong, Jin Kim, Jinhyun Jang, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Probabilistic_Prompt_Learning_for_Dense_Prediction_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2704",
      "paper_id": "",
      "title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning",
      "authors": "Shiming Chen, Wenjin Hou, Salman Khan, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Progressive_Semantic-Guided_Vision_Transformer_for_Zero-Shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2135",
      "paper_id": "",
      "title": "Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning",
      "authors": "Man Liu, Feng Li, Chunjie Zhang, Yunchao Wei, Huihui Bai, Yao Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2677",
      "paper_id": "",
      "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
      "authors": "Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Bendou_ProKeR_A_Kernel_Perspective_on_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_134",
      "paper_id": "",
      "title": "Prompt Distribution Learning",
      "authors": "Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1828",
      "paper_id": "",
      "title": "Prompt Learning via Meta-Regularization",
      "authors": "Jinyoung Park, Juyeon Ko, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Park_Prompt_Learning_via_Meta-Regularization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_199",
      "paper_id": "",
      "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners",
      "authors": "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2706",
      "paper_id": "",
      "title": "PromptHash:Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
      "authors": "Qiang Zou, Shuli Cheng, Jiayi Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zou_PromptHashAffinity-Prompted_Collaborative_Cross-Modal_Learning_for_Adaptive_Hashing_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1868",
      "paper_id": "",
      "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
      "authors": "Zheng Li, Xiang Li, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PromptKD_Unsupervised_Prompt_Distillation_for_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1446",
      "paper_id": "",
      "title": "ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues",
      "authors": "Hengcan Shi, Munawar Hayat, Yicheng Wu, Jianfei Cai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_ProposalCLIP_Unsupervised_Open-Category_Object_Proposal_Generation_via_Exploiting_CLIP_Cues_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2300",
      "paper_id": "",
      "title": "ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval",
      "authors": "Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao Li, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Fang_ProS_Prompting-to-simulate_Generalized_knowledge_for_Universal_Cross-Domain_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1127",
      "paper_id": "",
      "title": "ProTeCt: Prompt Tuning for Taxonomic Open Set Classification",
      "authors": "Tz-Ying Wu, Chih-Hui Ho, Nuno Vasconcelos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_ProTeCt_Prompt_Tuning_for_Taxonomic_Open_Set_Classification_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2148",
      "paper_id": "",
      "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning",
      "authors": "Cheng Chen, Yunpeng Zhai, Yifan Zhao, Jinyang Gao, Bolin Ding, Jia Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Provoking_Multi-modal_Few-Shot_LVLM_via_Exploration-Exploitation_In-Context_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_748",
      "paper_id": "",
      "title": "Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
      "authors": "Zaid Khan, Vijay Kumar BG, Samuel Schulter, Xiang Yu, Yun Fu, Manmohan Chandraker",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2815",
      "paper_id": "",
      "title": "Query Efficient Black-Box Visual Prompting with Subspace Learning",
      "authors": "Zhaogeng Liu, Haozhen Zhang, Hualin Zhang, Xingchen Li, Wanli Shi, Bin Gu, Yi Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Query_Efficient_Black-Box_Visual_Prompting_with_Subspace_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2188",
      "paper_id": "",
      "title": "Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model",
      "authors": "Tian Liang, Jing Huang, Ming Kong, Luyuan Chen, Qiang Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_211",
      "paper_id": "",
      "title": "RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training",
      "authors": "Chen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, Jingren Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1438",
      "paper_id": "",
      "title": "RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models",
      "authors": "Greg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, Pavlo Molchanov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Heinrich_RADIOv2.5_Improved_Baselines_for_Agglomerative_Vision_Foundation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_957",
      "paper_id": "",
      "title": "Realistic Test-Time Adaptation of Vision-Language Models",
      "authors": "Maxime Zanella, Clément Fuchs, Christophe De Vleeschouwer, Ismail Ben Ayed",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1458",
      "paper_id": "",
      "title": "Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval",
      "authors": "Yuanmin Tang, Jue Zhang, Xiaoting Qin, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_677",
      "paper_id": "",
      "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
      "authors": "Quanxing Zha, Xin Liu, Shu-Juan Peng, Yiu-ming Cheung, Xing Xu, Nannan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zha_ReCon_Enhancing_True_Correspondence_Discrimination_through_Relation_Consistency_for_Robust_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2250",
      "paper_id": "",
      "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport",
      "authors": "Hao Tan, Zichang Tan, Jun Li, Ajian Liu, Jun Wan, Zhen Lei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tan_Recover_and_Match_Open-Vocabulary_Multi-Label_Recognition_through_Knowledge-Constrained_Optimal_Transport_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1792",
      "paper_id": "",
      "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
      "authors": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Caffagni_Recurrence-Enhanced_Vision-and-Language_Transformers_for_Robust_Multimodal_Document_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1364",
      "paper_id": "",
      "title": "Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers",
      "authors": "Dahun Kim, Anelia Angelova, Weicheng Kuo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_694",
      "paper_id": "",
      "title": "RegionCLIP: Region-Based Language-Image Pretraining",
      "authors": "Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, Jianfeng Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_923",
      "paper_id": "",
      "title": "Replacing Labeled Real-Image Datasets With Auto-Generated Contours",
      "authors": "Hirokatsu Kataoka, Ryo Hayamizu, Ryosuke Yamada, Kodai Nakashima, Sora Takashima, Xinyu Zhang, Edgar Josafat Martinez-Noriega, Nakamasa Inoue, Rio Yokota",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kataoka_Replacing_Labeled_Real-Image_Datasets_With_Auto-Generated_Contours_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_320",
      "paper_id": "",
      "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
      "authors": "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_424",
      "paper_id": "",
      "title": "Reproducible Vision-Language Models Meet Concepts Out of Pre-Training",
      "authors": "Ziliang Chen, Xin Huang, Xiaoxuan Fan, Keze Wang, Yuyu Zhou, Quanlong Guan, Liang Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Reproducible_Vision-Language_Models_Meet_Concepts_Out_of_Pre-Training_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_858",
      "paper_id": "",
      "title": "ResCLIP: Residual Attention for Training-free Dense Vision-language Inference",
      "authors": "Yuhang Yang, Jinhong Deng, Wen Li, Lixin Duan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_ResCLIP_Residual_Attention_for_Training-free_Dense_Vision-language_Inference_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1084",
      "paper_id": "",
      "title": "Resource-Efficient Transformer Pruning for Finetuning of Large Models",
      "authors": "Fatih Ilhan, Gong Su, Selim Furkan Tekin, Tiansheng Huang, Sihao Hu, Ling Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1493",
      "paper_id": "",
      "title": "Retaining Knowledge and Enhancing Long-Text Representations in CLIP through Dual-Teacher Distillation",
      "authors": "Yuheng Feng, Changsong Wen, Zelin Peng, Li jiaye, Siyu Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_Retaining_Knowledge_and_Enhancing_Long-Text_Representations_in_CLIP_through_Dual-Teacher_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1673",
      "paper_id": "",
      "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
      "authors": "Matteo Farina, Massimiliano Mancini, Giovanni Iacca, Elisa Ricci",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Farina_Rethinking_Few-Shot_Adaptation_of_Vision-Language_Models_in_Two_Stages_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1982",
      "paper_id": "",
      "title": "Rethinking Token Reduction with Parameter-Efficient Fine-Tuning in ViT for Pixel-Level Tasks",
      "authors": "Cheng Lei, Ao Li, Hu Yao, Ce Zhu, Le Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lei_Rethinking_Token_Reduction_with_Parameter-Efficient_Fine-Tuning_in_ViT_for_Pixel-Level_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2265",
      "paper_id": "",
      "title": "Retrieval-Augmented Open-Vocabulary Object Detection",
      "authors": "Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1613",
      "paper_id": "",
      "title": "Revamping Cross-Modal Recipe Retrieval With Hierarchical Transformers and Self-Supervised Learning",
      "authors": "Amaia Salvador, Erhan Gundogdu, Loris Bazzani, Michael Donoser",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Salvador_Revamping_Cross-Modal_Recipe_Retrieval_With_Hierarchical_Transformers_and_Self-Supervised_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1758",
      "paper_id": "",
      "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
      "authors": "Yuxiao Chen, Jianbo Yuan, Yu Tian, Shijie Geng, Xinyu Li, Ding Zhou, Dimitris N. Metaxas, Hongxia Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Revisiting_Multimodal_Representation_in_Contrastive_Learning_From_Patch_and_Token_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_545",
      "paper_id": "",
      "title": "Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
      "authors": "Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, Laurens van der Maaten",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_Revisiting_Weakly_Supervised_Pre-Training_of_Visual_Perception_Models_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1943",
      "paper_id": "",
      "title": "RILS: Masked Visual Reconstruction in Language Semantic Space",
      "authors": "Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie, Xinggang Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_RILS_Masked_Visual_Reconstruction_in_Language_Semantic_Space_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1151",
      "paper_id": "",
      "title": "Robust Cross-Modal Representation Learning With Progressive Self-Distillation",
      "authors": "Alex Andonian, Shixing Chen, Raffay Hamid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_228",
      "paper_id": "",
      "title": "Robust Noisy Correspondence Learning with Equivariant Similarity Consistency",
      "authors": "Yuchen Yang, Likai Wang, Erkun Yang, Cheng Deng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Robust_Noisy_Correspondence_Learning_with_Equivariant_Similarity_Consistency_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1179",
      "paper_id": "",
      "title": "RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval",
      "authors": "Yanglin Feng, Hongyuan Zhu, Dezhong Peng, Xi Peng, Peng Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2015",
      "paper_id": "",
      "title": "SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training",
      "authors": "Sitong Wu, Haoru Tan, Zhuotao Tian, Yukang Chen, Xiaojuan Qi, Jiaya Jia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_SaCo_Loss_Sample-wise_Affinity_Consistency_for_Vision-Language_Pre-training_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2004",
      "paper_id": "",
      "title": "ScaleDet: A Scalable Multi-Dataset Object Detector",
      "authors": "Yanbei Chen, Manchen Wang, Abhay Mittal, Zhenlin Xu, Paolo Favaro, Joseph Tighe, Davide Modolo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2038",
      "paper_id": "",
      "title": "Scaling Language-Image Pre-Training via Masking",
      "authors": "Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_932",
      "paper_id": "",
      "title": "Scaling Laws for Data Filtering-- Data Curation cannot be Compute Agnostic",
      "authors": "Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, J. Zico Kolter",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Goyal_Scaling_Laws_for_Data_Filtering--_Data_Curation_cannot_be_Compute_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1050",
      "paper_id": "",
      "title": "Scaling Laws of Synthetic Images for Model Training ... for Now",
      "authors": "Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Scaling_Laws_of_Synthetic_Images_for_Model_Training_..._for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_2006",
      "paper_id": "",
      "title": "Scaling Up Vision-Language Pre-Training for Image Captioning",
      "authors": "Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Scaling_Up_Vision-Language_Pre-Training_for_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2207",
      "paper_id": "",
      "title": "Scaling Vision Pre-Training to 4K Resolution",
      "authors": "Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, Hongxu Yin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1248",
      "paper_id": "",
      "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting",
      "authors": "Chenyu Zhang, Kunlun Xu, Zichen Liu, Yuxin Peng, Jiahuan Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_SCAP_Transductive_Test-Time_Adaptation_via_Supportive_Clique-based_Attribute_Prompting_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2149",
      "paper_id": "",
      "title": "Scene-adaptive and Region-aware Multi-modal Prompt for Open Vocabulary Object Detection",
      "authors": "Xiaowei Zhao, Xianglong Liu, Duorui Wang, Yajun Gao, Zhide Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Scene-adaptive_and_Region-aware_Multi-modal_Prompt_for_Open_Vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_454",
      "paper_id": "",
      "title": "SceneTrilogy: On Human Scene-Sketch and Its Complementarity With Photo and Text",
      "authors": "Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1675",
      "paper_id": "",
      "title": "Scenic: A JAX Library for Computer Vision Research and Beyond",
      "authors": "Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, Yi Tay",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Dehghani_Scenic_A_JAX_Library_for_Computer_Vision_Research_and_Beyond_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1159",
      "paper_id": "",
      "title": "Search and Detect: Training-Free Long Tail Object Detection via Web-Image Retrieval",
      "authors": "Mankeerat Sidhu, Hetarth Chopra, Ansel Blume, Jeonghwan Kim, Revanth Gangi Reddy, Heng Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Sidhu_Search_and_Detect_Training-Free_Long_Tail_Object_Detection_via_Web-Image_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_804",
      "paper_id": "",
      "title": "Seeing More with Less: Human-like Representations in Vision Models",
      "authors": "Andrey Gizdov, Shimon Ullman, Daniel Harari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1382",
      "paper_id": "",
      "title": "Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learning",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Seeing_Out_of_the_Box_End-to-End_Pre-Training_for_Vision-Language_Representation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2493",
      "paper_id": "",
      "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models",
      "authors": "Davide Talon, Federico Girella, Ziyue Liu, Marco Cristani, Yiming Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Talon_Seeing_the_Abstract_Translating_the_Abstract_Language_for_Vision_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_964",
      "paper_id": "",
      "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
      "authors": "Gensheng Pei, Tao Chen, Yujia Wang, Xinhao Cai, Xiangbo Shu, Tianfei Zhou, Yazhou Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pei_Seeing_What_Matters_Empowering_CLIP_with_Patch_Generation-to-Selection_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_123",
      "paper_id": "",
      "title": "Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning",
      "authors": "Yatai Ji, Rongcheng Tu, Jie Jiang, Weijie Kong, Chengfei Cai, Wenzhe Zhao, Hongfa Wang, Yujiu Yang, Wei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Seeing_What_You_Miss_Vision-Language_Pre-Training_With_Semantic_Completion_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_292",
      "paper_id": "",
      "title": "Self-Evolving Visual Concept Library using Vision-Language Critics",
      "authors": "Atharva Sehgal, Patrick Yuan, Ziniu Hu, Yisong Yue, Jennifer J. Sun, Swarat Chaudhuri",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Sehgal_Self-Evolving_Visual_Concept_Library_using_Vision-Language_Critics_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_324",
      "paper_id": "",
      "title": "Semantic and Expressive Variations in Image Captions Across Languages",
      "authors": "Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_Semantic_and_Expressive_Variations_in_Image_Captions_Across_Languages_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_655",
      "paper_id": "",
      "title": "Sensitivity-Aware Efficient Fine-Tuning via Compact Dynamic-Rank Adaptation",
      "authors": "Tianran Chen, Jiarui Chen, Baoquan Zhang, Zhehao Yu, Shidong Chen, Rui Ye, Xutao Li, Yunming Ye",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Sensitivity-Aware_Efficient_Fine-Tuning_via_Compact_Dynamic-Rank_Adaptation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1906",
      "paper_id": "",
      "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models",
      "authors": "Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, Alexei A. Efros",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Sequential_Modeling_Enables_Scalable_Learning_for_Large_Vision_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2316",
      "paper_id": "",
      "title": "Sheared Backpropagation for Fine-tuning Foundation Models",
      "authors": "Zhiyuan Yu, Li Shen, Liang Ding, Xinmei Tian, Yixin Chen, Dacheng Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Sheared_Backpropagation_for_Fine-tuning_Foundation_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_820",
      "paper_id": "",
      "title": "SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection",
      "authors": "Mingxuan Liu, Tyler L. Hayes, Elisa Ricci, Gabriela Csurka, Riccardo Volpi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_SHiNe_Semantic_Hierarchy_Nexus_for_Open-vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_18",
      "paper_id": "",
      "title": "Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning",
      "authors": "Xiangyu Li, Xu Yang, Kun Wei, Cheng Deng, Muli Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Siamese_Contrastive_Embedding_Network_for_Compositional_Zero-Shot_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1494",
      "paper_id": "",
      "title": "Sieve: Multimodal Dataset Pruning using Image Captioning Models",
      "authors": "Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari S. Morcos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Mahmoud_Sieve_Multimodal_Dataset_Pruning_using_Image_Captioning_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1123",
      "paper_id": "",
      "title": "Simple Multi-Dataset Detection",
      "authors": "Xingyi Zhou, Vladlen Koltun, Philipp Krähenbühl",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Simple_Multi-Dataset_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_108",
      "paper_id": "",
      "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch",
      "authors": "Aneeshan Sain, Subhajit Maity, Pinaki Nath Chowdhury, Shubhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Sain_Sketch_Down_the_FLOPs_Towards_Efficient_Networks_for_Human_Sketch_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_908",
      "paper_id": "",
      "title": "Sketch2Saliency: Learning To Detect Salient Objects From Human Drawings",
      "authors": "Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Kumar, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bhunia_Sketch2Saliency_Learning_To_Detect_Salient_Objects_From_Human_Drawings_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1126",
      "paper_id": "",
      "title": "Sketch3T: Test-Time Training for Zero-Shot SBIR",
      "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Vaishnav Potlapalli, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sain_Sketch3T_Test-Time_Training_for_Zero-Shot_SBIR_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_299",
      "paper_id": "",
      "title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models",
      "authors": "Subhadeep Koley, Tapas Kumar Dutta, Aneeshan Sain, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Koley_SketchFusion_Learning_Universal_Sketch_Features_through_Fusing_Foundation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1549",
      "paper_id": "",
      "title": "Sketching Without Worrying: Noise-Tolerant Sketch-Based Image Retrieval",
      "authors": "Ayan Kumar Bhunia, Subhadeep Koley, Abdullah Faiz Ur Rahman Khilji, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bhunia_Sketching_Without_Worrying_Noise-Tolerant_Sketch-Based_Image_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2088",
      "paper_id": "",
      "title": "SketchXAI: A First Look at Explainability for Human Sketches",
      "authors": "Zhiyu Qu, Yulia Gryaditskaya, Ke Li, Kaiyue Pang, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_SketchXAI_A_First_Look_at_Explainability_for_Human_Sketches_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1473",
      "paper_id": "",
      "title": "Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves",
      "authors": "Shihan Wu, Ji Zhang, Pengpeng Zeng, Lianli Gao, Jingkuan Song, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Skip_Tuning_Pre-trained_Vision-Language_Models_are_Effective_and_Efficient_Adapters_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1258",
      "paper_id": "",
      "title": "SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation",
      "authors": "Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1304",
      "paper_id": "",
      "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
      "authors": "Shaoan Xie, Lingjing Kong, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_241",
      "paper_id": "",
      "title": "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models",
      "authors": "Kevin Miller, Aditya Gangrade, Samarth Mishra, Kate Saenko, Venkatesh Saligrama",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Miller_SPARC_Score_Prompting_and_Adaptive_Fusion_for_Zero-Shot_Multi-Label_Recognition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1932",
      "paper_id": "",
      "title": "Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization",
      "authors": "Zhi-Fan Wu, Chaojie Mao, Xue Wang, Jianwen Jiang, Yiliang Lv, Rong Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Structured_Model_Probing_Empowering_Efficient_Transfer_Learning_by_Structured_Regularization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_266",
      "paper_id": "",
      "title": "StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval",
      "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Sain_StyleMeUp_Towards_Style-Agnostic_Sketch-Based_Image_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_101",
      "paper_id": "",
      "title": "SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization",
      "authors": "Hongrui Jia, Chaoya Jiang, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jia_SymDPO_Boosting_In-Context_Learning_of_Large_Multimodal_Models_with_Symbol_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1353",
      "paper_id": "",
      "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
      "authors": "Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Song_SyncMask_Synchronized_Attentional_Masking_for_Fashion-centric_Vision-Language_Pretraining_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1844",
      "paper_id": "",
      "title": "TADFormer: Task-Adaptive Dynamic TransFormer for Efficient Multi-Task Learning",
      "authors": "Seungmin Baek, Soyul Lee, Hayeon Jo, Hyesong Choi, Dongbo Min",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Baek_TADFormer_Task-Adaptive_Dynamic_TransFormer_for_Efficient_Multi-Task_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2243",
      "paper_id": "",
      "title": "Taming Self-Training for Open-Vocabulary Object Detection",
      "authors": "Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar B G, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Taming_Self-Training_for_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_516",
      "paper_id": "",
      "title": "Task Adaptive Parameter Sharing for Multi-Task Learning",
      "authors": "Matthew Wallingford, Hao Li, Alessandro Achille, Avinash Ravichandran, Charless Fowlkes, Rahul Bhotika, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wallingford_Task_Adaptive_Parameter_Sharing_for_Multi-Task_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1117",
      "paper_id": "",
      "title": "Task Residual for Tuning Vision-Language Models",
      "authors": "Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_287",
      "paper_id": "",
      "title": "Task Singular Vectors: Reducing Task Interference in Model Merging",
      "authors": "Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, Emanuele Rodolà",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gargiulo_Task_Singular_Vectors_Reducing_Task_Interference_in_Model_Merging_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2130",
      "paper_id": "",
      "title": "Task-Aware Clustering for Prompting Vision-Language Models",
      "authors": "Fusheng Hao, Fengxiang He, Fuxiang Wu, Tichao Wang, Chengqun Song, Jun Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hao_Task-Aware_Clustering_for_Prompting_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_578",
      "paper_id": "",
      "title": "Task2Sim: Towards Effective Pre-Training and Transfer From Synthetic Data",
      "authors": "Samarth Mishra, Rameswar Panda, Cheng Perng Phoo, Chun-Fu (Richard) Chen, Leonid Karlinsky, Kate Saenko, Venkatesh Saligrama, Rogerio S. Feris",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Mishra_Task2Sim_Towards_Effective_Pre-Training_and_Transfer_From_Synthetic_Data_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1059",
      "paper_id": "",
      "title": "Taxonomy-Aware Evaluation of Vision-Language Models",
      "authors": "Vésteinn Snæbjarnarson, Kevin Du, Niklas Stoehr, Serge Belongie, Ryan Cotterell, Nico Lang, Stella Frank",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Snaebjarnarson_Taxonomy-Aware_Evaluation_of_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1211",
      "paper_id": "",
      "title": "TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model",
      "authors": "Hantao Yao, Rui Zhang, Changsheng Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_TCPTextual-based_Class-aware_Prompt_tuning_for_Visual-Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2322",
      "paper_id": "",
      "title": "Teaching Structured Vision & Language Concepts to Vision & Language Models",
      "authors": "Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, Leonid Karlinsky",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1883",
      "paper_id": "",
      "title": "Test-Time Visual In-Context Tuning",
      "authors": "Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, Bernt Schiele",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_Test-Time_Visual_In-Context_Tuning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1357",
      "paper_id": "",
      "title": "Text Augmented Correlation Transformer For Few-shot Classification & Segmentation",
      "authors": "Srinivasa Rao Nandam, Sara Atito, Zhenhua Feng, Josef Kittler, Muhammad Awais",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1314",
      "paper_id": "",
      "title": "Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers",
      "authors": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Koley_Text-to-Image_Diffusion_Models_are_Great_Sketch-Photo_Matchmakers_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2218",
      "paper_id": "",
      "title": "Texts as Images in Prompt Tuning for Multi-Label Image Recognition",
      "authors": "Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1475",
      "paper_id": "",
      "title": "The Devil is in the Fine-Grained Details: Evaluating Open-Vocabulary Object Detectors for Fine-Grained Understanding",
      "authors": "Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Claudio Gennaro, Fabrizio Falchi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bianchi_The_Devil_is_in_the_Fine-Grained_Details_Evaluating_Open-Vocabulary_Object_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1945",
      "paper_id": "",
      "title": "The Neglected Tails in Vision-Language Models",
      "authors": "Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Parashar_The_Neglected_Tails_in_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_466",
      "paper_id": "",
      "title": "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval With Transformers",
      "authors": "Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Miech_Thinking_Fast_and_Slow_Efficient_Text-to-Visual_Retrieval_With_Transformers_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1179",
      "paper_id": "",
      "title": "Time- Memory- and Parameter-Efficient Visual Adaptation",
      "authors": "Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Mercea_Time-_Memory-_and_Parameter-Efficient_Visual_Adaptation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2821",
      "paper_id": "",
      "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model",
      "authors": "Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_TopV_Compatible_Token_Pruning_with_Inference_Time_Optimization_for_Fast_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_63",
      "paper_id": "",
      "title": "Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture",
      "authors": "Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1329",
      "paper_id": "",
      "title": "Towards More Unified In-context Visual Understanding",
      "authors": "Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, Nenghai Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sheng_Towards_More_Unified_In-context_Visual_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1093",
      "paper_id": "",
      "title": "Train-Once-for-All Personalization",
      "authors": "Hong-You Chen, Yandong Li, Yin Cui, Mingda Zhang, Wei-Lun Chao, Li Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1974",
      "paper_id": "",
      "title": "Training-Free Pretrained Model Merging",
      "authors": "Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Training-Free_Pretrained_Model_Merging_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2451",
      "paper_id": "",
      "title": "Transductive Zero-Shot and Few-Shot CLIP",
      "authors": "Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, Jean-Christophe Pesquet, Ismail Ben Ayed",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2527",
      "paper_id": "",
      "title": "Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning",
      "authors": "Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Troika_Multi-Path_Cross-Modal_Traction_for_Compositional_Zero-Shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1680",
      "paper_id": "",
      "title": "Tune-An-Ellipse: CLIP Has Potential to Find What You Want",
      "authors": "Jinheng Xie, Songhe Deng, Bing Li, Haozhe Liu, Yawen Huang, Yefeng Zheng, Jurgen Schmidhuber, Bernard Ghanem, Linlin Shen, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_Tune-An-Ellipse_CLIP_Has_Potential_to_Find_What_You_Want_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_26",
      "paper_id": "",
      "title": "UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training",
      "authors": "Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu, Jingjing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_128",
      "paper_id": "",
      "title": "Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning",
      "authors": "Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, Trishul Chilimbi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1987",
      "paper_id": "",
      "title": "Understanding and Improving Visual Prompting: A Label-Mapping Perspective",
      "authors": "Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_772",
      "paper_id": "",
      "title": "Understanding Fine-tuning CLIP for Open-vocabulary Semantic Segmentation in Hyperbolic Space",
      "authors": "Zelin Peng, Zhengqin Xu, Zhilin Zeng, Changsong Wen, Yu Huang, Menglin Yang, Feilong Tang, Wei Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Peng_Understanding_Fine-tuning_CLIP_for_Open-vocabulary_Semantic_Segmentation_in_Hyperbolic_Space_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_483",
      "paper_id": "",
      "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks",
      "authors": "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_157",
      "paper_id": "",
      "title": "Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks",
      "authors": "Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2662",
      "paper_id": "",
      "title": "UNIALIGN: Scaling Multimodal Alignment within One Unified Model",
      "authors": "Bo Zhou, Liulei Li, Yujia Wang, Huafeng Liu, Yazhou Yao, Wenguan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_UNIALIGN_Scaling_Multimodal_Alignment_within_One_Unified_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1952",
      "paper_id": "",
      "title": "UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All",
      "authors": "Yuanhuiyi Lyu, Xu Zheng, Jiazhou Zhou, Lin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_UniBind_LLM-Augmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1216",
      "paper_id": "",
      "title": "Unified Contrastive Learning in Image-Text-Label Space",
      "authors": "Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2008",
      "paper_id": "",
      "title": "UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory",
      "authors": "Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, Long Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Diao_UniPT_Universal_Parallel_Tuning_for_Transfer_Learning_with_Efficient_Parameter_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_992",
      "paper_id": "",
      "title": "Universal Instance Perception As Object Discovery and Retrieval",
      "authors": "Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1314",
      "paper_id": "",
      "title": "Unseen Classes at a Later Time? No Problem",
      "authors": "Hari Chandana Kuchibhotla, Sumitra S Malagi, Shivam Chandhok, Vineeth N Balasubramanian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kuchibhotla_Unseen_Classes_at_a_Later_Time_No_Problem_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_590",
      "paper_id": "",
      "title": "Unsupervised Vision-and-Language Pre-Training via Retrieval-Based Multi-Granular Alignment",
      "authors": "Mingyang Zhou, Licheng Yu, Amanpreet Singh, Mengjiao Wang, Zhou Yu, Ning Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Unsupervised_Vision-and-Language_Pre-Training_via_Retrieval-Based_Multi-Granular_Alignment_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_534",
      "paper_id": "",
      "title": "Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs With Language Structures via Dependency Relationships",
      "authors": "Chao Lou, Wenjuan Han, Yuhuan Lin, Zilong Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lou_Unsupervised_Vision-Language_Parsing_Seamlessly_Bridging_Visual_Scene_Graphs_With_Language_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1096",
      "paper_id": "",
      "title": "VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning",
      "authors": "Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_VGSE_Visually-Grounded_Semantic_Embeddings_for_Zero-Shot_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_787",
      "paper_id": "",
      "title": "VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding",
      "authors": "Kangsan Kim, Geon Park, Youngwan Lee, Woongyeong Yeo, Sung Ju Hwang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_VideoICL_Confidence-based_Iterative_In-context_Learning_for_Out-of-Distribution_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1732",
      "paper_id": "",
      "title": "ViLEM: Visual-Language Error Modeling for Image-Text Retrieval",
      "authors": "Yuxin Chen, Zongyang Ma, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Weiming Hu, Xiaohu Qie, Jianping Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_89",
      "paper_id": "",
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1248",
      "paper_id": "",
      "title": "VirTex: Learning Visual Representations From Textual Annotations",
      "authors": "Karan Desai, Justin Johnson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Desai_VirTex_Learning_Visual_Representations_From_Textual_Annotations_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1996",
      "paper_id": "",
      "title": "Vision-Language Pre-Training With Triple Contrastive Learning",
      "authors": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Vision-Language_Pre-Training_With_Triple_Contrastive_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2216",
      "paper_id": "",
      "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
      "authors": "Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_VisionZip_Longer_is_Better_but_Not_Necessary_in_Vision_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1581",
      "paper_id": "",
      "title": "ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval",
      "authors": "Mengjun Cheng, Yipeng Sun, Longchao Wang, Xiongwei Zhu, Kun Yao, Jie Chen, Guoli Song, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_ViSTA_Vision_and_Scene_Text_Aggregation_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2112",
      "paper_id": "",
      "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning",
      "authors": "Huajie Jiang, Zhengxian Li, Xiaohan Yu, Yongli Hu, Baocai Yin, Jian Yang, Yuankai Qi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Visual_and_Semantic_Prompt_Collaboration_for_Generalized_Zero-Shot_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_706",
      "paper_id": "",
      "title": "Visual Atoms: Pre-Training Vision Transformers With Sinusoidal Waves",
      "authors": "Sora Takashima, Ryo Hayamizu, Nakamasa Inoue, Hirokatsu Kataoka, Rio Yokota",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1861",
      "paper_id": "",
      "title": "Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval",
      "authors": "Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, Ser-Nam Lim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jang_Visual_Delta_Generator_with_Large_Multi-modal_Models_for_Semi-supervised_Composed_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1539",
      "paper_id": "",
      "title": "Visual In-Context Prompting",
      "authors": "Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Visual_In-Context_Prompting_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2025",
      "paper_id": "",
      "title": "Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning",
      "authors": "Cheng-Hao Tu, Zheda Mai, Wei-Lun Chao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1842",
      "paper_id": "",
      "title": "Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning",
      "authors": "Wenjin Hou, Shiming Chen, Shuhuang Chen, Ziming Hong, Yan Wang, Xuetao Feng, Salman Khan, Fahad Shahbaz Khan, Xinge You",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Hou_Visual-Augmented_Dynamic_Semantic_Prototype_for_Generative_Zero-Shot_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_252",
      "paper_id": "",
      "title": "Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization",
      "authors": "Hantao Yao, Rui Zhang, Changsheng Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_519",
      "paper_id": "",
      "title": "VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning",
      "authors": "Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2162",
      "paper_id": "",
      "title": "ViT-Lens: Towards Omni-modal Representations",
      "authors": "Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_ViT-Lens_Towards_Omni-modal_Representations_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_153",
      "paper_id": "",
      "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
      "authors": "Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_265",
      "paper_id": "",
      "title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks",
      "authors": "Yi-Lin Sung, Jaemin Cho, Mohit Bansal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_516",
      "paper_id": "",
      "title": "VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks",
      "authors": "Jinseong Jang, Chunfei Ma, Byeongwon Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jang_VL2Lite_Task-Specific_Knowledge_Distillation_from_Large_Vision-Language_Models_to_Lightweight_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_381",
      "paper_id": "",
      "title": "VladVA: Discriminative Fine-tuning of LVLMs",
      "authors": "Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martinez, Georgios Tzimiropoulos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ouali_VladVA_Discriminative_Fine-tuning_of_LVLMs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2639",
      "paper_id": "",
      "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
      "authors": "Byung-Kwan Lee, Ryo Hachiuma, Yu-Chiang Frank Wang, Yong Man Ro, Yueh-Hua Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_VLsI_Verbalized_Layers-to-Interactions_from_Large_to_Small_Vision_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_837",
      "paper_id": "",
      "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
      "authors": "Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Yansong Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_VoCo-LLaMA_Towards_Vision_Compression_with_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_459",
      "paper_id": "",
      "title": "WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models",
      "authors": "Fu Feng, Yucheng Xie, Jing Wang, Xin Geng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_WAVE_Weight_Templates_for_Adaptive_Initialization_of_Variable-sized_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_877",
      "paper_id": "",
      "title": "What Can Human Sketches Do for Object Detection?",
      "authors": "Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chowdhury_What_Can_Human_Sketches_Do_for_Object_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2174",
      "paper_id": "",
      "title": "What Sketch Explainability Really Means for Downstream Tasks?",
      "authors": "Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bandyopadhyay_What_Sketch_Explainability_Really_Means_for_Downstream_Tasks_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2161",
      "paper_id": "",
      "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection",
      "authors": "Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_114",
      "paper_id": "",
      "title": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model",
      "authors": "Shengkun Tang, Yaqing Wang, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, Yi Liang, Dongkuan Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_You_Need_Multiple_Exiting_Dynamic_Early_Exiting_for_Accelerating_Unified_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2632",
      "paper_id": "",
      "title": "You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval",
      "authors": "Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Koley_Youll_Never_Walk_Alone_A_Sketch_and_Text_Duet_for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1673",
      "paper_id": "",
      "title": "ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation",
      "authors": "Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, Yifan Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1098",
      "paper_id": "",
      "title": "Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style",
      "authors": "Fengyin Lin, Mingkang Li, Da Li, Timothy Hospedales, Yi-Zhe Song, Yonggang Qi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Zero-Shot_Everything_Sketch-Based_Image_Retrieval_and_in_Explainable_Style_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_23",
      "paper_id": "",
      "title": "ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic",
      "authors": "Yoad Tewel, Yoav Shalev, Idan Schwartz, Lior Wolf",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Tewel_ZeroCap_Zero-Shot_Image-to-Text_Generation_for_Visual-Semantic_Arithmetic_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    }
  ]
}